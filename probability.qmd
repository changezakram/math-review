---
title: "Probability"
format: html
---

# Probability for Machine Learning

Probability is central to machine learning — particularly in generative models, classification, and Bayesian methods.

## Random Variables and Distributions

- **Discrete**: Bernoulli, Binomial
- **Continuous**: Normal (Gaussian), Exponential

Examples:

$$
P(X = x) \quad \text{(discrete)}, \qquad f_X(x) \quad \text{(continuous)}
$$

## Expectation and Variance

- **Expected value**:

$$
\mathbb{E}[X] = \sum_x x P(X=x) \quad \text{or} \quad \mathbb{E}[X] = \int x f_X(x)\, dx
$$

- **Variance**:

$$
\mathrm{Var}(X) = \mathbb{E}[(X - \mathbb{E}[X])^2]
$$

## Bayes’ Theorem

Used in Naive Bayes, probabilistic inference:

$$
P(A \mid B) = \frac{P(B \mid A)\, P(A)}{P(B)}
$$

## Probability Distributions

Probability distributions describe how values of a random variable are distributed.

- **Discrete**: Bernoulli, Binomial
- **Continuous**: Normal, Exponential

Example: PDF of normal distribution (1D)

$$
f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x - \mu)^2}{2\sigma^2}\right)
$$

**Multivariate Normal Distribution:**

When the random variable is a vector $x \in \mathbb{R}^d$, the Gaussian generalizes to:

$$
\mathcal{N}(x \mid \mu, \Sigma) = \frac{1}{(2\pi)^{d/2} |\Sigma|^{1/2}} \exp\left( -\frac{1}{2} (x - \mu)^T \Sigma^{-1} (x - \mu) \right)
$$

- $x$: $d$-dimensional vector  
- $\mu$: Mean vector  
- $\Sigma$: Covariance matrix (describes variance and correlation)  
- $|\Sigma|$: Determinant of the covariance matrix  
- $(x - \mu)^T \Sigma^{-1} (x - \mu)$: Generalized (Mahalanobis) distance

::: {.callout-tip title="What's the Mahalanobis Distance?"}
The term $(x - \mu)^T \Sigma^{-1} (x - \mu)$ measures how far $x$ is from the mean $\mu$, adjusted for the shape of the distribution. It’s like Euclidean distance, but scaled to account for direction-dependent variance.
:::

If $\Sigma = \sigma^2 I$, this reduces to:

$$
\mathcal{N}(x \mid \mu, \sigma^2 I) = \frac{1}{(2\pi \sigma^2)^{d/2}} \exp\left( -\frac{1}{2\sigma^2} \| x - \mu \|^2 \right)
$$


## Maximum Likelihood Estimation (MLE)

Used to estimate parameters of models:

$$
\hat{\theta}_{\text{MLE}} = \arg\max_\theta P(D \mid \theta)
$$

## KL Divergence

Measures how one distribution diverges from another:

$$
D_{\text{KL}}(P \parallel Q) = \sum_x P(x) \log \frac{P(x)}{Q(x)}
$$

Used in training VAEs and information-theoretic learning.

## Common ML Applications

- **Logistic regression**: based on Bernoulli distribution  
- **Variational autoencoders (VAEs)**: use probability distributions to encode data  
- **Bayesian models**: handle uncertainty via prior/posterior reasoning
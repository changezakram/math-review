[
  {
    "objectID": "probability.html",
    "href": "probability.html",
    "title": "Probability",
    "section": "",
    "text": "Probability is central to machine learning — particularly in generative models, classification, and Bayesian methods.\n\n\n\n\nDiscrete: Bernoulli, Binomial\nContinuous: Normal (Gaussian), Exponential\n\nExamples:\n\\[\nP(X = x) \\quad \\text{(discrete)}, \\qquad f_X(x) \\quad \\text{(continuous)}\n\\]\n\n\n\n\n\nExpected value:\n\n\\[\n\\mathbb{E}[X] = \\sum_x x P(X=x) \\quad \\text{or} \\quad \\mathbb{E}[X] = \\int x f_X(x)\\, dx\n\\]\n\n\nVariance:\n\n\\[\n\\mathrm{Var}(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2]\n\\]\n\n\n\n\nBayes’ Theorem lets us update what we believe about a situation after seeing new evidence.\nIt answers the question:\n“Given that something has happened (\\(B\\)), how likely is it that some other condition (\\(A\\)) was true?”\nThe formula is:\n\\[\nP(A \\mid B) = \\frac{P(B \\mid A) \\cdot P(A)}{P(B)}\n\\]\n\n\\(P(A \\mid B)\\): Posterior — the probability of \\(A\\) given that \\(B\\) happened\n\n\\(P(B \\mid A)\\): Likelihood — how likely is \\(B\\) if \\(A\\) is true\n\n\\(P(A)\\): Prior — our initial belief about \\(A\\)\n\n\\(P(B)\\): Evidence — total probability of \\(B\\) happening under all possibilities\n\n\n\n\n\n\n\nWhat is Bayes’ Theorem really doing?\n\n\n\nBayes’ Theorem is about updating your belief:\n\nStart with your prior belief (\\(P(A)\\))\nThen observe new evidence (\\(B\\))\nUpdate your belief using how likely that evidence is under \\(A\\) (\\(P(B \\mid A)\\))\n\n\n\n\n\nSuppose a disease affects 1% of the population.\nYou take a test that is: - \\(P(\\text{Positive} \\mid \\text{Disease}) = 0.99\\) (true positive rate) - \\(P(\\text{Positive} \\mid \\text{No Disease}) = 0.05\\) (false positive rate)\nYou test positive. What is the chance you actually have the disease?\nWe want to compute:\n\\[\nP(\\text{Disease} \\mid \\text{Positive}) = \\frac{P(\\text{Positive} \\mid \\text{Disease}) \\cdot P(\\text{Disease})}{P(\\text{Positive})}\n\\]\nLet’s plug in values: - \\(P(\\text{Disease}) = 0.01\\) - \\(P(\\text{No Disease}) = 0.99\\) - \\(P(\\text{Positive}) = 0.99 \\cdot 0.01 + 0.05 \\cdot 0.99 = 0.0594\\)\nSo:\n\\[\nP(\\text{Disease} \\mid \\text{Positive}) = \\frac{0.99 \\cdot 0.01}{0.0594} \\approx 0.167\n\\]\n\n\n\n\n\n\nSurprising result\n\n\n\nEven after a positive test, the chance of having the disease is only about 16.7%, because false positives are more common than true positives.\n\n\n\n\n\nBayes’ Theorem is widely used in:\n\nMedical diagnosis\n\nSpam detection\n\nProbabilistic machine learning (e.g., Naive Bayes classifiers)\n\nUpdating beliefs in AI models\n\n\n\n\n\n\n\nProportional version of Bayes’ Rule\n\n\n\nIn many machine learning applications, the denominator \\(P(B)\\) is the same for all outcomes and can be ignored:\n\\[\nP(A \\mid B) \\propto P(B \\mid A) \\cdot P(A)\n\\]\nThis version is often used for ranking outcomes instead of calculating exact probabilities.\n\n\n\n\n\n\n\nProbability distributions describe how values of a random variable are distributed.\n\nDiscrete: Bernoulli, Binomial\nContinuous: Normal, Exponential\n\nPDF of normal distribution in 1D\n\\[\nf(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)\n\\]\n\n\nThe normal (Gaussian) distribution can be extended to multiple variables — for example, when \\(x\\) is a vector instead of just a number.\nWhen \\(x\\) is a vector in \\(\\mathbb{R}^d\\) (i.e., a list of \\(d\\) values), the multivariate Gaussian looks like:\n\\[\n\\mathcal{N}(x \\mid \\mu, \\Sigma) =\n\\frac{1}{(2\\pi)^{d/2} \\, |\\Sigma|^{1/2}}\n\\exp\\left( -\\frac{1}{2}(x - \\mu)^T \\Sigma^{-1}(x - \\mu) \\right)\n\\]\nWhat do all these symbols mean?\n\n\\(x\\): A \\(d\\)-dimensional vector (e.g., an image flattened into a 1D array)\n\\(\\mu\\): The mean vector (the “center” of the distribution)\n\\(\\Sigma\\): The \\(d \\times d\\) covariance matrix, which captures the spread and correlation of the variables\n\\(|\\Sigma|\\): The determinant of \\(\\Sigma\\), representing the volume of the distribution\n\\((2\\pi)^{d/2}\\): Comes from extending the 1D normalizing constant to \\(d\\) dimensions\n\\((x - \\mu)^T \\Sigma^{-1}(x - \\mu)\\): A generalized squared distance from \\(x\\) to the mean — see below!\n\n\n\n\n\n\n\nWhat’s the Mahalanobis Distance?\n\n\n\nThe term \\((x - \\mu)^T \\Sigma^{-1}(x - \\mu)\\) measures how far \\(x\\) is from the mean \\(\\mu\\), taking into account how spread out the data is in different directions. It’s like Euclidean distance, but adjusted for direction-dependent variance. The more noise (variance) in a direction, the less the distance is penalized in that direction.\n\n\n\n\n\nIf all variables are independent and have equal variance \\(\\sigma^2\\), then \\(\\Sigma = \\sigma^2 I\\) and the formula simplifies to:\n\\[\n\\mathcal{N}(x \\mid \\mu, \\sigma^2 I) =\n\\frac{1}{(2\\pi \\sigma^2)^{d/2}}\n\\exp\\left( -\\frac{1}{2\\sigma^2} \\|x - \\mu\\|^2 \\right)\n\\]\nThis looks more like the familiar 1D bell curve — but extended to \\(d\\) dimensions.\n\n\n\n\nUsed to estimate parameters of models:\n\\[\n\\hat{\\theta}_{\\text{MLE}} = \\arg\\max_\\theta P(D \\mid \\theta)\n\\]\n\n\n\n\nMeasures how one distribution diverges from another:\n\\[\nD_{\\text{KL}}(P \\parallel Q) = \\sum_x P(x) \\log \\frac{P(x)}{Q(x)}\n\\]\nUsed in training VAEs and information-theoretic learning."
  },
  {
    "objectID": "probability.html#random-variables-and-distributions",
    "href": "probability.html#random-variables-and-distributions",
    "title": "Probability",
    "section": "",
    "text": "Discrete: Bernoulli, Binomial\nContinuous: Normal (Gaussian), Exponential\n\nExamples:\n\\[\nP(X = x) \\quad \\text{(discrete)}, \\qquad f_X(x) \\quad \\text{(continuous)}\n\\]"
  },
  {
    "objectID": "probability.html#expectation-and-variance",
    "href": "probability.html#expectation-and-variance",
    "title": "Probability",
    "section": "",
    "text": "Expected value:\n\n\\[\n\\mathbb{E}[X] = \\sum_x x P(X=x) \\quad \\text{or} \\quad \\mathbb{E}[X] = \\int x f_X(x)\\, dx\n\\]\n\n\nVariance:\n\n\\[\n\\mathrm{Var}(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2]\n\\]"
  },
  {
    "objectID": "probability.html#bayes-theorem",
    "href": "probability.html#bayes-theorem",
    "title": "Probability",
    "section": "",
    "text": "Bayes’ Theorem lets us update what we believe about a situation after seeing new evidence.\nIt answers the question:\n“Given that something has happened (\\(B\\)), how likely is it that some other condition (\\(A\\)) was true?”\nThe formula is:\n\\[\nP(A \\mid B) = \\frac{P(B \\mid A) \\cdot P(A)}{P(B)}\n\\]\n\n\\(P(A \\mid B)\\): Posterior — the probability of \\(A\\) given that \\(B\\) happened\n\n\\(P(B \\mid A)\\): Likelihood — how likely is \\(B\\) if \\(A\\) is true\n\n\\(P(A)\\): Prior — our initial belief about \\(A\\)\n\n\\(P(B)\\): Evidence — total probability of \\(B\\) happening under all possibilities\n\n\n\n\n\n\n\nWhat is Bayes’ Theorem really doing?\n\n\n\nBayes’ Theorem is about updating your belief:\n\nStart with your prior belief (\\(P(A)\\))\nThen observe new evidence (\\(B\\))\nUpdate your belief using how likely that evidence is under \\(A\\) (\\(P(B \\mid A)\\))\n\n\n\n\n\nSuppose a disease affects 1% of the population.\nYou take a test that is: - \\(P(\\text{Positive} \\mid \\text{Disease}) = 0.99\\) (true positive rate) - \\(P(\\text{Positive} \\mid \\text{No Disease}) = 0.05\\) (false positive rate)\nYou test positive. What is the chance you actually have the disease?\nWe want to compute:\n\\[\nP(\\text{Disease} \\mid \\text{Positive}) = \\frac{P(\\text{Positive} \\mid \\text{Disease}) \\cdot P(\\text{Disease})}{P(\\text{Positive})}\n\\]\nLet’s plug in values: - \\(P(\\text{Disease}) = 0.01\\) - \\(P(\\text{No Disease}) = 0.99\\) - \\(P(\\text{Positive}) = 0.99 \\cdot 0.01 + 0.05 \\cdot 0.99 = 0.0594\\)\nSo:\n\\[\nP(\\text{Disease} \\mid \\text{Positive}) = \\frac{0.99 \\cdot 0.01}{0.0594} \\approx 0.167\n\\]\n\n\n\n\n\n\nSurprising result\n\n\n\nEven after a positive test, the chance of having the disease is only about 16.7%, because false positives are more common than true positives.\n\n\n\n\n\nBayes’ Theorem is widely used in:\n\nMedical diagnosis\n\nSpam detection\n\nProbabilistic machine learning (e.g., Naive Bayes classifiers)\n\nUpdating beliefs in AI models\n\n\n\n\n\n\n\nProportional version of Bayes’ Rule\n\n\n\nIn many machine learning applications, the denominator \\(P(B)\\) is the same for all outcomes and can be ignored:\n\\[\nP(A \\mid B) \\propto P(B \\mid A) \\cdot P(A)\n\\]\nThis version is often used for ranking outcomes instead of calculating exact probabilities."
  },
  {
    "objectID": "probability.html#probability-distributions",
    "href": "probability.html#probability-distributions",
    "title": "Probability",
    "section": "",
    "text": "Probability distributions describe how values of a random variable are distributed.\n\nDiscrete: Bernoulli, Binomial\nContinuous: Normal, Exponential\n\nPDF of normal distribution in 1D\n\\[\nf(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)\n\\]\n\n\nThe normal (Gaussian) distribution can be extended to multiple variables — for example, when \\(x\\) is a vector instead of just a number.\nWhen \\(x\\) is a vector in \\(\\mathbb{R}^d\\) (i.e., a list of \\(d\\) values), the multivariate Gaussian looks like:\n\\[\n\\mathcal{N}(x \\mid \\mu, \\Sigma) =\n\\frac{1}{(2\\pi)^{d/2} \\, |\\Sigma|^{1/2}}\n\\exp\\left( -\\frac{1}{2}(x - \\mu)^T \\Sigma^{-1}(x - \\mu) \\right)\n\\]\nWhat do all these symbols mean?\n\n\\(x\\): A \\(d\\)-dimensional vector (e.g., an image flattened into a 1D array)\n\\(\\mu\\): The mean vector (the “center” of the distribution)\n\\(\\Sigma\\): The \\(d \\times d\\) covariance matrix, which captures the spread and correlation of the variables\n\\(|\\Sigma|\\): The determinant of \\(\\Sigma\\), representing the volume of the distribution\n\\((2\\pi)^{d/2}\\): Comes from extending the 1D normalizing constant to \\(d\\) dimensions\n\\((x - \\mu)^T \\Sigma^{-1}(x - \\mu)\\): A generalized squared distance from \\(x\\) to the mean — see below!\n\n\n\n\n\n\n\nWhat’s the Mahalanobis Distance?\n\n\n\nThe term \\((x - \\mu)^T \\Sigma^{-1}(x - \\mu)\\) measures how far \\(x\\) is from the mean \\(\\mu\\), taking into account how spread out the data is in different directions. It’s like Euclidean distance, but adjusted for direction-dependent variance. The more noise (variance) in a direction, the less the distance is penalized in that direction.\n\n\n\n\n\nIf all variables are independent and have equal variance \\(\\sigma^2\\), then \\(\\Sigma = \\sigma^2 I\\) and the formula simplifies to:\n\\[\n\\mathcal{N}(x \\mid \\mu, \\sigma^2 I) =\n\\frac{1}{(2\\pi \\sigma^2)^{d/2}}\n\\exp\\left( -\\frac{1}{2\\sigma^2} \\|x - \\mu\\|^2 \\right)\n\\]\nThis looks more like the familiar 1D bell curve — but extended to \\(d\\) dimensions."
  },
  {
    "objectID": "probability.html#maximum-likelihood-estimation-mle",
    "href": "probability.html#maximum-likelihood-estimation-mle",
    "title": "Probability",
    "section": "",
    "text": "Used to estimate parameters of models:\n\\[\n\\hat{\\theta}_{\\text{MLE}} = \\arg\\max_\\theta P(D \\mid \\theta)\n\\]"
  },
  {
    "objectID": "probability.html#kl-divergence",
    "href": "probability.html#kl-divergence",
    "title": "Probability",
    "section": "",
    "text": "Measures how one distribution diverges from another:\n\\[\nD_{\\text{KL}}(P \\parallel Q) = \\sum_x P(x) \\log \\frac{P(x)}{Q(x)}\n\\]\nUsed in training VAEs and information-theoretic learning."
  },
  {
    "objectID": "calculus.html",
    "href": "calculus.html",
    "title": "Calculus",
    "section": "",
    "text": "This section covers core calculus concepts used in machine learning — especially in optimization, backpropagation, and probability.\n\n\nA general method for rewriting quadratic expressions:\n\\[\na x^2 - 2 b x = a \\left( x - \\frac{b}{a} \\right)^2 - \\frac{b^2}{a}\n\\]\nThis transformation is widely used in:\n\nGaussian probability density derivations\n\nKL divergence simplification\n\nOptimization of quadratic objectives\n\n\n\n\nThe derivative of a function measures the rate of change:\n\\[\n\\frac{d}{dx} f(x)\n\\]\n\n\n\nUsed when dealing with multivariate functions:\n\\[\n\\frac{\\partial f}{\\partial x}, \\quad \\frac{\\partial f}{\\partial y}\n\\]\n\n\n\nFor a vector-valued function ( : ^n ^m ), the Jacobian is:\n\\[\nJ_{ij} = \\frac{\\partial f_i}{\\partial x_j}\n\\]\n\n\n\nSecond-order partial derivatives:\n\\[\nH_{ij} = \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}\n\\]\nUsed in curvature analysis and second-order optimization.\n\n\n\nArea under a curve:\n\\[\n\\int_a^b f(x) \\, dx\n\\]\n\n\n\nA useful transformation:\n\\[\n\\int u \\, dv = uv - \\int v \\, du\n\\]"
  },
  {
    "objectID": "calculus.html#completing-the-square",
    "href": "calculus.html#completing-the-square",
    "title": "Calculus",
    "section": "",
    "text": "A general method for rewriting quadratic expressions:\n\\[\na x^2 - 2 b x = a \\left( x - \\frac{b}{a} \\right)^2 - \\frac{b^2}{a}\n\\]\nThis transformation is widely used in:\n\nGaussian probability density derivations\n\nKL divergence simplification\n\nOptimization of quadratic objectives"
  },
  {
    "objectID": "calculus.html#derivatives",
    "href": "calculus.html#derivatives",
    "title": "Calculus",
    "section": "",
    "text": "The derivative of a function measures the rate of change:\n\\[\n\\frac{d}{dx} f(x)\n\\]"
  },
  {
    "objectID": "calculus.html#partial-derivatives",
    "href": "calculus.html#partial-derivatives",
    "title": "Calculus",
    "section": "",
    "text": "Used when dealing with multivariate functions:\n\\[\n\\frac{\\partial f}{\\partial x}, \\quad \\frac{\\partial f}{\\partial y}\n\\]"
  },
  {
    "objectID": "calculus.html#jacobian-matrix",
    "href": "calculus.html#jacobian-matrix",
    "title": "Calculus",
    "section": "",
    "text": "For a vector-valued function ( : ^n ^m ), the Jacobian is:\n\\[\nJ_{ij} = \\frac{\\partial f_i}{\\partial x_j}\n\\]"
  },
  {
    "objectID": "calculus.html#hessian-matrix",
    "href": "calculus.html#hessian-matrix",
    "title": "Calculus",
    "section": "",
    "text": "Second-order partial derivatives:\n\\[\nH_{ij} = \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}\n\\]\nUsed in curvature analysis and second-order optimization."
  },
  {
    "objectID": "calculus.html#integration",
    "href": "calculus.html#integration",
    "title": "Calculus",
    "section": "",
    "text": "Area under a curve:\n\\[\n\\int_a^b f(x) \\, dx\n\\]"
  },
  {
    "objectID": "calculus.html#integration-by-parts",
    "href": "calculus.html#integration-by-parts",
    "title": "Calculus",
    "section": "",
    "text": "A useful transformation:\n\\[\n\\int u \\, dv = uv - \\int v \\, du\n\\]"
  },
  {
    "objectID": "linear-algebra.html",
    "href": "linear-algebra.html",
    "title": "Linear Algebra",
    "section": "",
    "text": "This section covers essential linear concepts that serve as the mathematical foundation for machine learning and deep learning.\n\n\n\nVector: A 1D array of numbers, often used to represent features.\nMatrix: A 2D array of numbers, common for representing datasets and weights.\n\nExample:\n\\[\n\\mathbf{x} = \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix}, \\quad\n\\mathbf{W} = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\n\\]\n\n\n\nUsed in the forward pass of neural networks:\n\\[\n\\mathbf{y} = \\mathbf{W}\\mathbf{x}\n\\]\n\n\n\nMeasures projection or similarity:\n\\[\n\\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i} a_i b_i\n\\]\nGeometric interpretation:\n\\[\n\\mathbf{a} \\cdot \\mathbf{b} = \\|\\mathbf{a}\\| \\|\\mathbf{b}\\| \\cos(\\theta)\n\\]\n\nUsed in cosine similarity for comparing vectors\n\n\n\n\n\nL2 norm (Euclidean length): \\[\n\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_i x_i^2}\n\\]\nL1 norm (Manhattan distance): \\[\n\\|\\mathbf{x}\\|_1 = \\sum_i |x_i|\n\\]\nFrobenius norm (for matrices): \\[\n\\|\\mathbf{A}\\|_F = \\sqrt{\\sum_{i,j} A_{ij}^2}\n\\]\n\nUsed to compute distances and regularization penalties.\n\n\n\nThe determinant of a square matrix \\(\\mathbf{A}\\) is a scalar value:\n\\[\n\\det(\\mathbf{A})\n\\]\n\n\\(\\det(\\mathbf{A}) = 0\\) means matrix is singular (non-invertible)\n\\(|\\det(\\mathbf{A})|\\) represents volume scaling factor\nUsed in linear transformations and invertibility checks\n\n\n\n\nThe trace of a square matrix is the sum of diagonal elements:\n\\[\n\\text{Tr}(\\mathbf{A}) = \\sum_i A_{ii}\n\\]\nProperties:\n\n\\(\\text{Tr}(\\mathbf{A} + \\mathbf{B}) = \\text{Tr}(\\mathbf{A}) + \\text{Tr}(\\mathbf{B})\\)\n\\(\\text{Tr}(\\mathbf{AB}) = \\text{Tr}(\\mathbf{BA})\\) (cyclic property)\n\\(\\text{Tr}(\\mathbf{A}) = \\sum_i \\lambda_i\\) (sum of eigenvalues)\n\n\n\n\nUsed in PCA (dimensionality reduction):\n\\[\n\\mathbf{A}\\mathbf{v} = \\lambda\\mathbf{v}\n\\]\nWhere:\n\n\\(\\mathbf{v}\\) is an eigenvector\n\\(\\lambda\\) is the corresponding eigenvalue\nCharacteristic equation: \\(\\det(\\mathbf{A} - \\lambda\\mathbf{I}) = 0\\)\n\nProperties:\n\n\\(\\text{Tr}(\\mathbf{A}) = \\sum_i \\lambda_i\\) (sum of eigenvalues)\n\\(\\det(\\mathbf{A}) = \\prod_i \\lambda_i\\) (product of eigenvalues)\n\n\n\n\nUsed in matrix factorization and recommender systems:\n\\[\n\\mathbf{A} = \\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^T\n\\]\nWhere:\n\n\\(\\mathbf{U}\\): left singular vectors (m × m orthogonal)\n\\(\\mathbf{\\Sigma}\\): diagonal matrix of singular values (m × n)\n\\(\\mathbf{V}\\): right singular vectors (n × n orthogonal)\n\nKey properties:\n\nAlways exists (unlike eigendecomposition)\nSingular values \\(\\sigma_i \\geq 0\\), ordered by magnitude\nLow-rank approximation: keep top \\(k\\) singular values\n\nApplications:\n\nDimensionality reduction (truncated SVD)\nImage compression\nCollaborative filtering\nPseudo-inverse calculation\n\n\n\n\n\nRank: The dimension of the column space (or row space):\n\\[\n\\text{rank}(\\mathbf{A}) = \\text{number of linearly independent columns}\n\\]\nProperties:\n\n\\(\\text{rank}(\\mathbf{A}) \\leq \\min(m, n)\\) for \\(m \\times n\\) matrix\nFull rank: \\(\\text{rank}(\\mathbf{A}) = \\min(m, n)\\)\nRank-deficient: \\(\\text{rank}(\\mathbf{A}) &lt; \\min(m, n)\\)\n\nInvertibility:\n\nSquare matrix \\(\\mathbf{A}\\) is invertible \\(\\Leftrightarrow\\) \\(\\text{rank}(\\mathbf{A}) = n\\)\n\\(\\mathbf{A}^{-1}\\) exists \\(\\Leftrightarrow\\) \\(\\det(\\mathbf{A}) \\neq 0\\)\nFor invertible \\(\\mathbf{A}\\): \\(\\mathbf{AA}^{-1} = \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}\\)\n\nML relevance: Feature redundancy, linear dependence in datasets\n\n\n\nOrthogonal vectors: \\(\\mathbf{x} \\perp \\mathbf{y}\\) if \\(\\mathbf{x}^T\\mathbf{y} = 0\\)\nOrthogonal matrix \\(\\mathbf{Q}\\):\n\n\\(\\mathbf{Q}^T\\mathbf{Q} = \\mathbf{QQ}^T = \\mathbf{I}\\)\nColumns are orthonormal\nPreserves lengths: \\(\\|\\mathbf{Qx}\\| = \\|\\mathbf{x}\\|\\)\n\nProjection matrix \\(\\mathbf{P}\\):\nProjection of \\(\\mathbf{b}\\) onto column space of \\(\\mathbf{A}\\):\n\\[\n\\mathbf{P} = \\mathbf{A}(\\mathbf{A}^T\\mathbf{A})^{-1}\\mathbf{A}^T\n\\]\nProperties:\n\n\\(\\mathbf{P}^2 = \\mathbf{P}\\) (idempotent)\n\\(\\mathbf{P}^T = \\mathbf{P}\\) (symmetric)\n\nQR Decomposition:\n\\[\n\\mathbf{A} = \\mathbf{QR}\n\\]\nWhere \\(\\mathbf{Q}\\) is orthogonal, \\(\\mathbf{R}\\) is upper triangular\nML relevance:\n\nLinear regression (least squares)\nGram-Schmidt orthogonalization\nSolving linear systems\n\n\n\n\nA symmetric matrix \\(\\mathbf{A}\\) is positive definite if:\n\\[\n\\mathbf{x}^T\\mathbf{A}\\mathbf{x} &gt; 0 \\text{ for all } \\mathbf{x} \\neq \\mathbf{0}\n\\]\nEquivalent conditions:\n\nAll eigenvalues \\(\\lambda_i &gt; 0\\)\nAll leading principal minors \\(&gt; 0\\)\n\\(\\mathbf{A} = \\mathbf{B}^T\\mathbf{B}\\) for some invertible \\(\\mathbf{B}\\)\n\nPositive semi-definite: \\(\\mathbf{x}^T\\mathbf{A}\\mathbf{x} \\geq 0\\) (allows zero eigenvalues)\nML relevance:\n\nCovariance matrices (always positive semi-definite)\nHessian matrices in optimization\nConvex optimization (positive definite Hessian → strict convexity)\nConvergence guarantees\n\n\n\n\nGradient (scalar function w.r.t vector):\n\\[\n\\nabla_\\mathbf{x} f(\\mathbf{x}) = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_n} \\end{bmatrix}\n\\]\nJacobian (vector function w.r.t vector):\n\\[\n\\mathbf{J} = \\begin{bmatrix} \\frac{\\partial f_i}{\\partial x_j} \\end{bmatrix} \\text{ (m × n matrix)}\n\\]\nCommon derivatives:\n\n\\(\\nabla_\\mathbf{x}(\\mathbf{a}^T\\mathbf{x}) = \\mathbf{a}\\)\n\\(\\nabla_\\mathbf{x}(\\mathbf{x}^T\\mathbf{A}\\mathbf{x}) = (\\mathbf{A} + \\mathbf{A}^T)\\mathbf{x}\\)\nFor symmetric \\(\\mathbf{A}\\): \\(\\nabla_\\mathbf{x}(\\mathbf{x}^T\\mathbf{A}\\mathbf{x}) = 2\\mathbf{A}\\mathbf{x}\\)\n\\(\\nabla_\\mathbf{x}(\\|\\mathbf{x}\\|^2) = 2\\mathbf{x}\\)\n\nHessian (second derivatives):\n\\[\n\\mathbf{H} = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_i \\partial x_j} \\end{bmatrix}\n\\]\nML relevance:\n\nBackpropagation\nGradient descent optimization\nNeural network training\n\n\n\n\nFor symmetric matrix \\(\\mathbf{A}\\):\n\\[\n\\mathbf{A} = \\mathbf{Q}\\mathbf{\\Lambda}\\mathbf{Q}^T\n\\]\nWhere:\n\n\\(\\mathbf{Q}\\): orthogonal matrix of eigenvectors\n\\(\\mathbf{\\Lambda}\\): diagonal matrix of eigenvalues\n\\(\\mathbf{Q}^T = \\mathbf{Q}^{-1}\\)\n\nProperties:\n\nAll eigenvalues are real\nEigenvectors are orthogonal\nCan always be diagonalized\n\nML relevance:\n\nPrincipal Component Analysis (PCA)\nCovariance matrix diagonalization\nQuadratic forms"
  },
  {
    "objectID": "linear-algebra.html#vectors-and-matrices",
    "href": "linear-algebra.html#vectors-and-matrices",
    "title": "Linear Algebra",
    "section": "",
    "text": "Vector: A 1D array of numbers, often used to represent features.\nMatrix: A 2D array of numbers, common for representing datasets and weights.\n\nExample:\n\\[\n\\mathbf{x} = \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix}, \\quad\n\\mathbf{W} = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "linear-algebra.html#matrix-multiplication",
    "href": "linear-algebra.html#matrix-multiplication",
    "title": "Linear Algebra",
    "section": "",
    "text": "Used in the forward pass of neural networks:\n\\[\n\\mathbf{y} = \\mathbf{W}\\mathbf{x}\n\\]"
  },
  {
    "objectID": "linear-algebra.html#dot-product",
    "href": "linear-algebra.html#dot-product",
    "title": "Linear Algebra",
    "section": "",
    "text": "Measures projection or similarity:\n\\[\n\\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i} a_i b_i\n\\]\nGeometric interpretation:\n\\[\n\\mathbf{a} \\cdot \\mathbf{b} = \\|\\mathbf{a}\\| \\|\\mathbf{b}\\| \\cos(\\theta)\n\\]\n\nUsed in cosine similarity for comparing vectors"
  },
  {
    "objectID": "linear-algebra.html#norms-and-distances",
    "href": "linear-algebra.html#norms-and-distances",
    "title": "Linear Algebra",
    "section": "",
    "text": "L2 norm (Euclidean length): \\[\n\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_i x_i^2}\n\\]\nL1 norm (Manhattan distance): \\[\n\\|\\mathbf{x}\\|_1 = \\sum_i |x_i|\n\\]\nFrobenius norm (for matrices): \\[\n\\|\\mathbf{A}\\|_F = \\sqrt{\\sum_{i,j} A_{ij}^2}\n\\]\n\nUsed to compute distances and regularization penalties."
  },
  {
    "objectID": "linear-algebra.html#determinant",
    "href": "linear-algebra.html#determinant",
    "title": "Linear Algebra",
    "section": "",
    "text": "The determinant of a square matrix \\(\\mathbf{A}\\) is a scalar value:\n\\[\n\\det(\\mathbf{A})\n\\]\n\n\\(\\det(\\mathbf{A}) = 0\\) means matrix is singular (non-invertible)\n\\(|\\det(\\mathbf{A})|\\) represents volume scaling factor\nUsed in linear transformations and invertibility checks"
  },
  {
    "objectID": "linear-algebra.html#trace",
    "href": "linear-algebra.html#trace",
    "title": "Linear Algebra",
    "section": "",
    "text": "The trace of a square matrix is the sum of diagonal elements:\n\\[\n\\text{Tr}(\\mathbf{A}) = \\sum_i A_{ii}\n\\]\nProperties:\n\n\\(\\text{Tr}(\\mathbf{A} + \\mathbf{B}) = \\text{Tr}(\\mathbf{A}) + \\text{Tr}(\\mathbf{B})\\)\n\\(\\text{Tr}(\\mathbf{AB}) = \\text{Tr}(\\mathbf{BA})\\) (cyclic property)\n\\(\\text{Tr}(\\mathbf{A}) = \\sum_i \\lambda_i\\) (sum of eigenvalues)"
  },
  {
    "objectID": "linear-algebra.html#eigenvalues-and-eigenvectors",
    "href": "linear-algebra.html#eigenvalues-and-eigenvectors",
    "title": "Linear Algebra",
    "section": "",
    "text": "Used in PCA (dimensionality reduction):\n\\[\n\\mathbf{A}\\mathbf{v} = \\lambda\\mathbf{v}\n\\]\nWhere:\n\n\\(\\mathbf{v}\\) is an eigenvector\n\\(\\lambda\\) is the corresponding eigenvalue\nCharacteristic equation: \\(\\det(\\mathbf{A} - \\lambda\\mathbf{I}) = 0\\)\n\nProperties:\n\n\\(\\text{Tr}(\\mathbf{A}) = \\sum_i \\lambda_i\\) (sum of eigenvalues)\n\\(\\det(\\mathbf{A}) = \\prod_i \\lambda_i\\) (product of eigenvalues)"
  },
  {
    "objectID": "linear-algebra.html#singular-value-decomposition-svd",
    "href": "linear-algebra.html#singular-value-decomposition-svd",
    "title": "Linear Algebra",
    "section": "",
    "text": "Used in matrix factorization and recommender systems:\n\\[\n\\mathbf{A} = \\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^T\n\\]\nWhere:\n\n\\(\\mathbf{U}\\): left singular vectors (m × m orthogonal)\n\\(\\mathbf{\\Sigma}\\): diagonal matrix of singular values (m × n)\n\\(\\mathbf{V}\\): right singular vectors (n × n orthogonal)\n\nKey properties:\n\nAlways exists (unlike eigendecomposition)\nSingular values \\(\\sigma_i \\geq 0\\), ordered by magnitude\nLow-rank approximation: keep top \\(k\\) singular values\n\nApplications:\n\nDimensionality reduction (truncated SVD)\nImage compression\nCollaborative filtering\nPseudo-inverse calculation"
  },
  {
    "objectID": "linear-algebra.html#matrix-rank-and-invertibility",
    "href": "linear-algebra.html#matrix-rank-and-invertibility",
    "title": "Linear Algebra",
    "section": "",
    "text": "Rank: The dimension of the column space (or row space):\n\\[\n\\text{rank}(\\mathbf{A}) = \\text{number of linearly independent columns}\n\\]\nProperties:\n\n\\(\\text{rank}(\\mathbf{A}) \\leq \\min(m, n)\\) for \\(m \\times n\\) matrix\nFull rank: \\(\\text{rank}(\\mathbf{A}) = \\min(m, n)\\)\nRank-deficient: \\(\\text{rank}(\\mathbf{A}) &lt; \\min(m, n)\\)\n\nInvertibility:\n\nSquare matrix \\(\\mathbf{A}\\) is invertible \\(\\Leftrightarrow\\) \\(\\text{rank}(\\mathbf{A}) = n\\)\n\\(\\mathbf{A}^{-1}\\) exists \\(\\Leftrightarrow\\) \\(\\det(\\mathbf{A}) \\neq 0\\)\nFor invertible \\(\\mathbf{A}\\): \\(\\mathbf{AA}^{-1} = \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}\\)\n\nML relevance: Feature redundancy, linear dependence in datasets"
  },
  {
    "objectID": "linear-algebra.html#orthogonality-and-projections",
    "href": "linear-algebra.html#orthogonality-and-projections",
    "title": "Linear Algebra",
    "section": "",
    "text": "Orthogonal vectors: \\(\\mathbf{x} \\perp \\mathbf{y}\\) if \\(\\mathbf{x}^T\\mathbf{y} = 0\\)\nOrthogonal matrix \\(\\mathbf{Q}\\):\n\n\\(\\mathbf{Q}^T\\mathbf{Q} = \\mathbf{QQ}^T = \\mathbf{I}\\)\nColumns are orthonormal\nPreserves lengths: \\(\\|\\mathbf{Qx}\\| = \\|\\mathbf{x}\\|\\)\n\nProjection matrix \\(\\mathbf{P}\\):\nProjection of \\(\\mathbf{b}\\) onto column space of \\(\\mathbf{A}\\):\n\\[\n\\mathbf{P} = \\mathbf{A}(\\mathbf{A}^T\\mathbf{A})^{-1}\\mathbf{A}^T\n\\]\nProperties:\n\n\\(\\mathbf{P}^2 = \\mathbf{P}\\) (idempotent)\n\\(\\mathbf{P}^T = \\mathbf{P}\\) (symmetric)\n\nQR Decomposition:\n\\[\n\\mathbf{A} = \\mathbf{QR}\n\\]\nWhere \\(\\mathbf{Q}\\) is orthogonal, \\(\\mathbf{R}\\) is upper triangular\nML relevance:\n\nLinear regression (least squares)\nGram-Schmidt orthogonalization\nSolving linear systems"
  },
  {
    "objectID": "linear-algebra.html#positive-definite-matrices",
    "href": "linear-algebra.html#positive-definite-matrices",
    "title": "Linear Algebra",
    "section": "",
    "text": "A symmetric matrix \\(\\mathbf{A}\\) is positive definite if:\n\\[\n\\mathbf{x}^T\\mathbf{A}\\mathbf{x} &gt; 0 \\text{ for all } \\mathbf{x} \\neq \\mathbf{0}\n\\]\nEquivalent conditions:\n\nAll eigenvalues \\(\\lambda_i &gt; 0\\)\nAll leading principal minors \\(&gt; 0\\)\n\\(\\mathbf{A} = \\mathbf{B}^T\\mathbf{B}\\) for some invertible \\(\\mathbf{B}\\)\n\nPositive semi-definite: \\(\\mathbf{x}^T\\mathbf{A}\\mathbf{x} \\geq 0\\) (allows zero eigenvalues)\nML relevance:\n\nCovariance matrices (always positive semi-definite)\nHessian matrices in optimization\nConvex optimization (positive definite Hessian → strict convexity)\nConvergence guarantees"
  },
  {
    "objectID": "linear-algebra.html#matrix-calculus-basics",
    "href": "linear-algebra.html#matrix-calculus-basics",
    "title": "Linear Algebra",
    "section": "",
    "text": "Gradient (scalar function w.r.t vector):\n\\[\n\\nabla_\\mathbf{x} f(\\mathbf{x}) = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_n} \\end{bmatrix}\n\\]\nJacobian (vector function w.r.t vector):\n\\[\n\\mathbf{J} = \\begin{bmatrix} \\frac{\\partial f_i}{\\partial x_j} \\end{bmatrix} \\text{ (m × n matrix)}\n\\]\nCommon derivatives:\n\n\\(\\nabla_\\mathbf{x}(\\mathbf{a}^T\\mathbf{x}) = \\mathbf{a}\\)\n\\(\\nabla_\\mathbf{x}(\\mathbf{x}^T\\mathbf{A}\\mathbf{x}) = (\\mathbf{A} + \\mathbf{A}^T)\\mathbf{x}\\)\nFor symmetric \\(\\mathbf{A}\\): \\(\\nabla_\\mathbf{x}(\\mathbf{x}^T\\mathbf{A}\\mathbf{x}) = 2\\mathbf{A}\\mathbf{x}\\)\n\\(\\nabla_\\mathbf{x}(\\|\\mathbf{x}\\|^2) = 2\\mathbf{x}\\)\n\nHessian (second derivatives):\n\\[\n\\mathbf{H} = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_i \\partial x_j} \\end{bmatrix}\n\\]\nML relevance:\n\nBackpropagation\nGradient descent optimization\nNeural network training"
  },
  {
    "objectID": "linear-algebra.html#spectral-theorem",
    "href": "linear-algebra.html#spectral-theorem",
    "title": "Linear Algebra",
    "section": "",
    "text": "For symmetric matrix \\(\\mathbf{A}\\):\n\\[\n\\mathbf{A} = \\mathbf{Q}\\mathbf{\\Lambda}\\mathbf{Q}^T\n\\]\nWhere:\n\n\\(\\mathbf{Q}\\): orthogonal matrix of eigenvectors\n\\(\\mathbf{\\Lambda}\\): diagonal matrix of eigenvalues\n\\(\\mathbf{Q}^T = \\mathbf{Q}^{-1}\\)\n\nProperties:\n\nAll eigenvalues are real\nEigenvectors are orthogonal\nCan always be diagonalized\n\nML relevance:\n\nPrincipal Component Analysis (PCA)\nCovariance matrix diagonalization\nQuadratic forms"
  }
]
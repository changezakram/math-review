[
  {
    "objectID": "probability.html",
    "href": "probability.html",
    "title": "Probability",
    "section": "",
    "text": "Probability is central to machine learning — particularly in generative models, classification, and Bayesian methods.\n\n\n\n\nDiscrete: Bernoulli, Binomial\nContinuous: Normal (Gaussian), Exponential\n\nExamples:\n\\[\nP(X = x) \\quad \\text{(discrete)}, \\qquad f_X(x) \\quad \\text{(continuous)}\n\\]\n\n\n\n\n\nExpected value:\n\n\\[\n\\mathbb{E}[X] = \\sum_x x P(X=x) \\quad \\text{or} \\quad \\mathbb{E}[X] = \\int x f_X(x)\\, dx\n\\]\n\n\nVariance:\n\n\\[\n\\mathrm{Var}(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2]\n\\]\n\n\n\n\nBayes’ Theorem lets us update what we believe about a situation after seeing new evidence.\nIt answers the question:\n“Given that something has happened (\\(B\\)), how likely is it that some other condition (\\(A\\)) was true?”\nThe formula is:\n\\[\nP(A \\mid B) = \\frac{P(B \\mid A) \\cdot P(A)}{P(B)}\n\\]\n\n\\(P(A \\mid B)\\): Posterior — the probability of \\(A\\) given that \\(B\\) happened\n\n\\(P(B \\mid A)\\): Likelihood — how likely is \\(B\\) if \\(A\\) is true\n\n\\(P(A)\\): Prior — our initial belief about \\(A\\)\n\n\\(P(B)\\): Evidence — total probability of \\(B\\) happening under all possibilities\n\n\n\n\n\n\n\nWhat is Bayes’ Theorem really doing?\n\n\n\nBayes’ Theorem is about updating your belief:\n\nStart with your prior belief (\\(P(A)\\))\nThen observe new evidence (\\(B\\))\nUpdate your belief using how likely that evidence is under \\(A\\) (\\(P(B \\mid A)\\))\n\n\n\n\n\nSuppose a disease affects 1% of the population.\nYou take a test that is: - \\(P(\\text{Positive} \\mid \\text{Disease}) = 0.99\\) (true positive rate) - \\(P(\\text{Positive} \\mid \\text{No Disease}) = 0.05\\) (false positive rate)\nYou test positive. What is the chance you actually have the disease?\nWe want to compute:\n\\[\nP(\\text{Disease} \\mid \\text{Positive}) = \\frac{P(\\text{Positive} \\mid \\text{Disease}) \\cdot P(\\text{Disease})}{P(\\text{Positive})}\n\\]\nLet’s plug in values: - \\(P(\\text{Disease}) = 0.01\\) - \\(P(\\text{No Disease}) = 0.99\\) - \\(P(\\text{Positive}) = 0.99 \\cdot 0.01 + 0.05 \\cdot 0.99 = 0.0594\\)\nSo:\n\\[\nP(\\text{Disease} \\mid \\text{Positive}) = \\frac{0.99 \\cdot 0.01}{0.0594} \\approx 0.167\n\\]\n\n\n\n\n\n\nSurprising result\n\n\n\nEven after a positive test, the chance of having the disease is only about 16.7%, because false positives are more common than true positives.\n\n\n\n\n\nBayes’ Theorem is widely used in:\n\nMedical diagnosis\n\nSpam detection\n\nProbabilistic machine learning (e.g., Naive Bayes classifiers)\n\nUpdating beliefs in AI models\n\n\n\n\n\n\n\nProportional version of Bayes’ Rule\n\n\n\nIn many machine learning applications, the denominator \\(P(B)\\) is the same for all outcomes and can be ignored:\n\\[\nP(A \\mid B) \\propto P(B \\mid A) \\cdot P(A)\n\\]\nThis version is often used for ranking outcomes instead of calculating exact probabilities.\n\n\n\n\n\n\n\nProbability distributions describe how values of a random variable are distributed.\n\nDiscrete: Bernoulli, Binomial\nContinuous: Normal, Exponential\n\nPDF of normal distribution in 1D\n\\[\nf(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)\n\\]\n\n\nThe normal (Gaussian) distribution can be extended to multiple variables — for example, when \\(x\\) is a vector instead of just a number.\nWhen \\(x\\) is a vector in \\(\\mathbb{R}^d\\) (i.e., a list of \\(d\\) values), the multivariate Gaussian looks like:\n\\[\n\\mathcal{N}(x \\mid \\mu, \\Sigma) =\n\\frac{1}{(2\\pi)^{d/2} \\, |\\Sigma|^{1/2}}\n\\exp\\left( -\\frac{1}{2}(x - \\mu)^T \\Sigma^{-1}(x - \\mu) \\right)\n\\]\nWhat do all these symbols mean?\n\n\\(x\\): A \\(d\\)-dimensional vector (e.g., an image flattened into a 1D array)\n\\(\\mu\\): The mean vector (the “center” of the distribution)\n\\(\\Sigma\\): The \\(d \\times d\\) covariance matrix, which captures the spread and correlation of the variables\n\\(|\\Sigma|\\): The determinant of \\(\\Sigma\\), representing the volume of the distribution\n\\((2\\pi)^{d/2}\\): Comes from extending the 1D normalizing constant to \\(d\\) dimensions\n\\((x - \\mu)^T \\Sigma^{-1}(x - \\mu)\\): A generalized squared distance from \\(x\\) to the mean — see below!\n\n\n\n\n\n\n\nWhat’s the Mahalanobis Distance?\n\n\n\nThe term \\((x - \\mu)^T \\Sigma^{-1}(x - \\mu)\\) measures how far \\(x\\) is from the mean \\(\\mu\\), taking into account how spread out the data is in different directions. It’s like Euclidean distance, but adjusted for direction-dependent variance. The more noise (variance) in a direction, the less the distance is penalized in that direction.\n\n\n\n\n\nIf all variables are independent and have equal variance \\(\\sigma^2\\), then \\(\\Sigma = \\sigma^2 I\\) and the formula simplifies to:\n\\[\n\\mathcal{N}(x \\mid \\mu, \\sigma^2 I) =\n\\frac{1}{(2\\pi \\sigma^2)^{d/2}}\n\\exp\\left( -\\frac{1}{2\\sigma^2} \\|x - \\mu\\|^2 \\right)\n\\]\nThis looks more like the familiar 1D bell curve — but extended to \\(d\\) dimensions.\n\n\n\n\nUsed to estimate parameters of models:\n\\[\n\\hat{\\theta}_{\\text{MLE}} = \\arg\\max_\\theta P(D \\mid \\theta)\n\\]\n\n\n\n\nMeasures how one distribution diverges from another:\n\\[\nD_{\\text{KL}}(P \\parallel Q) = \\sum_x P(x) \\log \\frac{P(x)}{Q(x)}\n\\]\nUsed in training VAEs and information-theoretic learning."
  },
  {
    "objectID": "probability.html#random-variables-and-distributions",
    "href": "probability.html#random-variables-and-distributions",
    "title": "Probability",
    "section": "",
    "text": "Discrete: Bernoulli, Binomial\nContinuous: Normal (Gaussian), Exponential\n\nExamples:\n\\[\nP(X = x) \\quad \\text{(discrete)}, \\qquad f_X(x) \\quad \\text{(continuous)}\n\\]"
  },
  {
    "objectID": "probability.html#expectation-and-variance",
    "href": "probability.html#expectation-and-variance",
    "title": "Probability",
    "section": "",
    "text": "Expected value:\n\n\\[\n\\mathbb{E}[X] = \\sum_x x P(X=x) \\quad \\text{or} \\quad \\mathbb{E}[X] = \\int x f_X(x)\\, dx\n\\]\n\n\nVariance:\n\n\\[\n\\mathrm{Var}(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2]\n\\]"
  },
  {
    "objectID": "probability.html#bayes-theorem",
    "href": "probability.html#bayes-theorem",
    "title": "Probability",
    "section": "",
    "text": "Bayes’ Theorem lets us update what we believe about a situation after seeing new evidence.\nIt answers the question:\n“Given that something has happened (\\(B\\)), how likely is it that some other condition (\\(A\\)) was true?”\nThe formula is:\n\\[\nP(A \\mid B) = \\frac{P(B \\mid A) \\cdot P(A)}{P(B)}\n\\]\n\n\\(P(A \\mid B)\\): Posterior — the probability of \\(A\\) given that \\(B\\) happened\n\n\\(P(B \\mid A)\\): Likelihood — how likely is \\(B\\) if \\(A\\) is true\n\n\\(P(A)\\): Prior — our initial belief about \\(A\\)\n\n\\(P(B)\\): Evidence — total probability of \\(B\\) happening under all possibilities\n\n\n\n\n\n\n\nWhat is Bayes’ Theorem really doing?\n\n\n\nBayes’ Theorem is about updating your belief:\n\nStart with your prior belief (\\(P(A)\\))\nThen observe new evidence (\\(B\\))\nUpdate your belief using how likely that evidence is under \\(A\\) (\\(P(B \\mid A)\\))\n\n\n\n\n\nSuppose a disease affects 1% of the population.\nYou take a test that is: - \\(P(\\text{Positive} \\mid \\text{Disease}) = 0.99\\) (true positive rate) - \\(P(\\text{Positive} \\mid \\text{No Disease}) = 0.05\\) (false positive rate)\nYou test positive. What is the chance you actually have the disease?\nWe want to compute:\n\\[\nP(\\text{Disease} \\mid \\text{Positive}) = \\frac{P(\\text{Positive} \\mid \\text{Disease}) \\cdot P(\\text{Disease})}{P(\\text{Positive})}\n\\]\nLet’s plug in values: - \\(P(\\text{Disease}) = 0.01\\) - \\(P(\\text{No Disease}) = 0.99\\) - \\(P(\\text{Positive}) = 0.99 \\cdot 0.01 + 0.05 \\cdot 0.99 = 0.0594\\)\nSo:\n\\[\nP(\\text{Disease} \\mid \\text{Positive}) = \\frac{0.99 \\cdot 0.01}{0.0594} \\approx 0.167\n\\]\n\n\n\n\n\n\nSurprising result\n\n\n\nEven after a positive test, the chance of having the disease is only about 16.7%, because false positives are more common than true positives.\n\n\n\n\n\nBayes’ Theorem is widely used in:\n\nMedical diagnosis\n\nSpam detection\n\nProbabilistic machine learning (e.g., Naive Bayes classifiers)\n\nUpdating beliefs in AI models\n\n\n\n\n\n\n\nProportional version of Bayes’ Rule\n\n\n\nIn many machine learning applications, the denominator \\(P(B)\\) is the same for all outcomes and can be ignored:\n\\[\nP(A \\mid B) \\propto P(B \\mid A) \\cdot P(A)\n\\]\nThis version is often used for ranking outcomes instead of calculating exact probabilities."
  },
  {
    "objectID": "probability.html#probability-distributions",
    "href": "probability.html#probability-distributions",
    "title": "Probability",
    "section": "",
    "text": "Probability distributions describe how values of a random variable are distributed.\n\nDiscrete: Bernoulli, Binomial\nContinuous: Normal, Exponential\n\nPDF of normal distribution in 1D\n\\[\nf(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)\n\\]\n\n\nThe normal (Gaussian) distribution can be extended to multiple variables — for example, when \\(x\\) is a vector instead of just a number.\nWhen \\(x\\) is a vector in \\(\\mathbb{R}^d\\) (i.e., a list of \\(d\\) values), the multivariate Gaussian looks like:\n\\[\n\\mathcal{N}(x \\mid \\mu, \\Sigma) =\n\\frac{1}{(2\\pi)^{d/2} \\, |\\Sigma|^{1/2}}\n\\exp\\left( -\\frac{1}{2}(x - \\mu)^T \\Sigma^{-1}(x - \\mu) \\right)\n\\]\nWhat do all these symbols mean?\n\n\\(x\\): A \\(d\\)-dimensional vector (e.g., an image flattened into a 1D array)\n\\(\\mu\\): The mean vector (the “center” of the distribution)\n\\(\\Sigma\\): The \\(d \\times d\\) covariance matrix, which captures the spread and correlation of the variables\n\\(|\\Sigma|\\): The determinant of \\(\\Sigma\\), representing the volume of the distribution\n\\((2\\pi)^{d/2}\\): Comes from extending the 1D normalizing constant to \\(d\\) dimensions\n\\((x - \\mu)^T \\Sigma^{-1}(x - \\mu)\\): A generalized squared distance from \\(x\\) to the mean — see below!\n\n\n\n\n\n\n\nWhat’s the Mahalanobis Distance?\n\n\n\nThe term \\((x - \\mu)^T \\Sigma^{-1}(x - \\mu)\\) measures how far \\(x\\) is from the mean \\(\\mu\\), taking into account how spread out the data is in different directions. It’s like Euclidean distance, but adjusted for direction-dependent variance. The more noise (variance) in a direction, the less the distance is penalized in that direction.\n\n\n\n\n\nIf all variables are independent and have equal variance \\(\\sigma^2\\), then \\(\\Sigma = \\sigma^2 I\\) and the formula simplifies to:\n\\[\n\\mathcal{N}(x \\mid \\mu, \\sigma^2 I) =\n\\frac{1}{(2\\pi \\sigma^2)^{d/2}}\n\\exp\\left( -\\frac{1}{2\\sigma^2} \\|x - \\mu\\|^2 \\right)\n\\]\nThis looks more like the familiar 1D bell curve — but extended to \\(d\\) dimensions."
  },
  {
    "objectID": "probability.html#maximum-likelihood-estimation-mle",
    "href": "probability.html#maximum-likelihood-estimation-mle",
    "title": "Probability",
    "section": "",
    "text": "Used to estimate parameters of models:\n\\[\n\\hat{\\theta}_{\\text{MLE}} = \\arg\\max_\\theta P(D \\mid \\theta)\n\\]"
  },
  {
    "objectID": "probability.html#kl-divergence",
    "href": "probability.html#kl-divergence",
    "title": "Probability",
    "section": "",
    "text": "Measures how one distribution diverges from another:\n\\[\nD_{\\text{KL}}(P \\parallel Q) = \\sum_x P(x) \\log \\frac{P(x)}{Q(x)}\n\\]\nUsed in training VAEs and information-theoretic learning."
  },
  {
    "objectID": "calculus.html",
    "href": "calculus.html",
    "title": "Calculus",
    "section": "",
    "text": "This section covers core calculus concepts used in machine learning — especially in optimization, backpropagation, and probability.\n\n\nA general method for rewriting quadratic expressions:\n\\[\nx^2 - 2bx + c = \\left(x - b\\right)^2 + \\frac{b^2}{c} - \\frac{b^2}{a}\n\\]\nThis transformation is widely used in:\n\nGaussian probability density derivations\nKL divergence simplification\nOptimization of quadratic objectives\n\n\n\n\nThe derivative of a function measures the rate of change:\n\\[\n\\frac{d}{dx} f(x)\n\\]\nCommon derivatives:\n\n\\(\\frac{d}{dx}(x^n) = nx^{n-1}\\) (power rule)\n\\(\\frac{d}{dx}(e^x) = e^x\\)\n\\(\\frac{d}{dx}(\\ln x) = \\frac{1}{x}\\)\n\\(\\frac{d}{dx}(\\sin x) = \\cos x\\)\n\\(\\frac{d}{dx}(\\cos x) = -\\sin x\\)\n\\(\\frac{d}{dx}(\\tanh x) = 1 - \\tanh^2 x\\) (used in activation functions)\n\nRules:\n\nProduct rule: \\((fg)' = f'g + fg'\\)\nQuotient rule: \\(\\left(\\frac{f}{g}\\right)' = \\frac{f'g - fg'}{g^2}\\)\nChain rule: \\(\\frac{d}{dx}f(g(x)) = f'(g(x)) \\cdot g'(x)\\)\n\n\n\n\nUsed when dealing with multivariable functions:\n\\[\n\\frac{\\partial f}{\\partial x}, \\quad \\frac{\\partial f}{\\partial y}\n\\]\nExample: For \\(f(x,y) = x^2y + 3y^2\\): - \\(\\frac{\\partial f}{\\partial x} = 2xy\\) - \\(\\frac{\\partial f}{\\partial y} = x^2 + 6y\\)\nML relevance: Computing gradients for each parameter in neural networks\n\n\n\nThe gradient is a vector of all partial derivatives:\n\\[\n\\nabla f = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_n} \\end{bmatrix}\n\\]\nProperties:\n\nPoints in direction of steepest ascent\nMagnitude indicates steepness\nPerpendicular to level curves/surfaces\n\nML relevance: Gradient descent uses \\(-\\nabla f\\) to minimize loss functions\n\n\n\nFor a vector-valued function \\(\\mathbf{f}: \\mathbb{R}^n \\to \\mathbb{R}^m\\), the Jacobian is:\n\\[\nJ_{ij} = \\frac{\\partial f_i}{\\partial x_j}\n\\]\nDimensions: \\(m \\times n\\) matrix\nExample: For \\(\\mathbf{f}(x,y) = [x^2y, xy^2]\\):\n\\[\nJ = \\begin{bmatrix}\n2xy & x^2 \\\\\ny^2 & 2xy\n\\end{bmatrix}\n\\]\nML relevance:\n\nBackpropagation through layers\nComputing derivatives of vector outputs\n\n\n\n\nSecond-order partial derivatives:\n\\[\nH_{ij} = \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}\n\\]\nProperties:\n\nSymmetric matrix (if \\(f\\) is twice differentiable)\nDiagonal elements: \\(\\frac{\\partial^2 f}{\\partial x_i^2}\\)\nOff-diagonal: mixed partials \\(\\frac{\\partial^2 f}{\\partial x_i \\partial x_j}\\)\n\nUsed in curvature analysis and second-order optimization.\nML relevance:\n\nNewton’s method optimization\nAnalyzing convergence properties\nIdentifying saddle points vs local minima\n\n\n\n\nFor composed functions \\(f(g(x))\\):\n\\[\n\\frac{\\partial f}{\\partial x_i} = \\sum_j \\frac{\\partial f}{\\partial g_j} \\cdot \\frac{\\partial g_j}{\\partial x_i}\n\\]\nMatrix form:\n\\[\n\\frac{\\partial f}{\\partial \\mathbf{x}} = \\frac{\\partial f}{\\partial \\mathbf{g}} \\cdot \\frac{\\partial \\mathbf{g}}{\\partial \\mathbf{x}}\n\\]\nML relevance: Foundation of backpropagation algorithm\n\n\n\nRate of change of \\(f\\) in direction of unit vector \\(\\mathbf{u}\\):\n\\[\nD_\\mathbf{u} f = \\nabla f \\cdot \\mathbf{u}\n\\]\nProperties:\n\nMaximum when \\(\\mathbf{u}\\) aligned with \\(\\nabla f\\)\nZero when \\(\\mathbf{u}\\) perpendicular to \\(\\nabla f\\)\n\nML relevance: Understanding optimization landscapes\n\n\n\nApproximating functions using derivatives:\n\\[\nf(x + \\Delta x) \\approx f(x) + f'(x)\\Delta x + \\frac{1}{2}f''(x)\\Delta x^2 + \\cdots\n\\]\nFirst-order (linear) approximation:\n\\[\nf(x + \\Delta x) \\approx f(x) + f'(x)\\Delta x\n\\]\nSecond-order (quadratic) approximation:\n\\[\nf(x + \\Delta x) \\approx f(x) + f'(x)\\Delta x + \\frac{1}{2}f''(x)\\Delta x^2\n\\]\nMultivariate Taylor expansion:\n\\[\nf(\\mathbf{x} + \\mathbf{\\Delta x}) \\approx f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^T \\mathbf{\\Delta x} + \\frac{1}{2}\\mathbf{\\Delta x}^T H \\mathbf{\\Delta x}\n\\]\nML relevance:\n\nLocal approximations in optimization\nNewton’s method derivation\nTrust region methods\n\n\n\n\nArea under a curve:\n\\[\n\\int_a^b f(x) \\, dx\n\\]\nCommon integrals:\n\n\\(\\int x^n \\, dx = \\frac{x^{n+1}}{n+1} + C\\) (for \\(n \\neq -1\\))\n\\(\\int e^x \\, dx = e^x + C\\)\n\\(\\int \\frac{1}{x} \\, dx = \\ln|x| + C\\)\n\\(\\int \\sin x \\, dx = -\\cos x + C\\)\n\\(\\int \\cos x \\, dx = \\sin x + C\\)\n\nML relevance:\n\nComputing expectations in probability\nNormalizing probability distributions\nDeriving closed-form solutions\n\n\n\n\nA useful transformation:\n\\[\n\\int u \\, dv = uv - \\int v \\, du\n\\]\nML relevance:\n\nDeriving variational bounds\nEvidence lower bound (ELBO) derivations\n\n\n\n\nLinks differentiation and integration:\n\\[\n\\frac{d}{dx} \\int_a^x f(t) \\, dt = f(x)\n\\]\n\\[\n\\int_a^b f'(x) \\, dx = f(b) - f(a)\n\\]\n\n\n\n\n\nAt a local minimum/maximum \\(x^*\\):\n\\[\n\\nabla f(x^*) = \\mathbf{0}\n\\]\nCritical points: Where gradient vanishes\n\n\n\nFor a local minimum at \\(x^*\\):\n\n\\(\\nabla f(x^*) = \\mathbf{0}\\) (first-order condition)\nHessian \\(H(x^*)\\) is positive definite (all eigenvalues &gt; 0)\n\nFor a local maximum: - Hessian \\(H(x^*)\\) is negative definite (all eigenvalues &lt; 0)\nSaddle point: Hessian has both positive and negative eigenvalues\nML relevance: Analyzing loss function landscapes\n\n\n\n\nA function \\(f\\) is convex if:\n\\[\nf(\\lambda x + (1-\\lambda)y) \\leq \\lambda f(x) + (1-\\lambda)f(y)\n\\]\nfor all \\(x, y\\) and \\(\\lambda \\in [0,1]\\)\nEquivalent conditions:\n\n\\(f''(x) \\geq 0\\) (for univariate functions)\nHessian \\(H\\) is positive semi-definite (for multivariate)\nAny local minimum is a global minimum\n\nStrictly convex: Use strict inequalities (\\(&gt;\\) instead of \\(\\geq\\))\nML relevance:\n\nGuarantees for optimization convergence\nLinear regression, logistic regression are convex\nNeural networks are generally non-convex\n\n\n\n\nFor constrained optimization:\n\\[\n\\min f(\\mathbf{x}) \\quad \\text{subject to} \\quad g(\\mathbf{x}) = 0\n\\]\nMethod: Solve:\n\\[\n\\nabla f(\\mathbf{x}) = \\lambda \\nabla g(\\mathbf{x})\n\\]\nand \\(g(\\mathbf{x}) = 0\\)\nLagrangian:\n\\[\n\\mathcal{L}(\\mathbf{x}, \\lambda) = f(\\mathbf{x}) - \\lambda g(\\mathbf{x})\n\\]\nML relevance:\n\nSupport Vector Machines (SVM)\nConstrained optimization problems\nDual formulations\n\n\n\n\nFor indeterminate forms \\(\\frac{0}{0}\\) or \\(\\frac{\\infty}{\\infty}\\):\n\\[\n\\lim_{x \\to c} \\frac{f(x)}{g(x)} = \\lim_{x \\to c} \\frac{f'(x)}{g'(x)}\n\\]\nML relevance: Analyzing limiting behavior of loss functions\n\n\n\nSigmoid: \\(\\sigma(x) = \\frac{1}{1 + e^{-x}}\\)\n\\[\n\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))\n\\]\nTanh: \\(\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\\)\n\\[\n\\tanh'(x) = 1 - \\tanh^2(x)\n\\]\nReLU: \\(\\text{ReLU}(x) = \\max(0, x)\\)\n\\[\n\\text{ReLU}'(x) = \\begin{cases} 1 & \\text{if } x &gt; 0 \\\\ 0 & \\text{if } x &lt; 0 \\\\ \\text{undefined} & \\text{if } x = 0 \\end{cases}\n\\]\nSoftmax: For vector \\(\\mathbf{z}\\), \\(\\text{softmax}(\\mathbf{z})_i = \\frac{e^{z_i}}{\\sum_j e^{z_j}}\\)\n\\[\n\\frac{\\partial \\text{softmax}(\\mathbf{z})_i}{\\partial z_j} = \\text{softmax}(\\mathbf{z})_i (\\delta_{ij} - \\text{softmax}(\\mathbf{z})_j)\n\\]\nwhere \\(\\delta_{ij}\\) is the Kronecker delta"
  },
  {
    "objectID": "calculus.html#completing-the-square",
    "href": "calculus.html#completing-the-square",
    "title": "Calculus",
    "section": "",
    "text": "A general method for rewriting quadratic expressions:\n\\[\nx^2 - 2bx + c = \\left(x - b\\right)^2 + \\frac{b^2}{c} - \\frac{b^2}{a}\n\\]\nThis transformation is widely used in:\n\nGaussian probability density derivations\nKL divergence simplification\nOptimization of quadratic objectives"
  },
  {
    "objectID": "calculus.html#derivatives",
    "href": "calculus.html#derivatives",
    "title": "Calculus",
    "section": "",
    "text": "The derivative of a function measures the rate of change:\n\\[\n\\frac{d}{dx} f(x)\n\\]\nCommon derivatives:\n\n\\(\\frac{d}{dx}(x^n) = nx^{n-1}\\) (power rule)\n\\(\\frac{d}{dx}(e^x) = e^x\\)\n\\(\\frac{d}{dx}(\\ln x) = \\frac{1}{x}\\)\n\\(\\frac{d}{dx}(\\sin x) = \\cos x\\)\n\\(\\frac{d}{dx}(\\cos x) = -\\sin x\\)\n\\(\\frac{d}{dx}(\\tanh x) = 1 - \\tanh^2 x\\) (used in activation functions)\n\nRules:\n\nProduct rule: \\((fg)' = f'g + fg'\\)\nQuotient rule: \\(\\left(\\frac{f}{g}\\right)' = \\frac{f'g - fg'}{g^2}\\)\nChain rule: \\(\\frac{d}{dx}f(g(x)) = f'(g(x)) \\cdot g'(x)\\)"
  },
  {
    "objectID": "calculus.html#partial-derivatives",
    "href": "calculus.html#partial-derivatives",
    "title": "Calculus",
    "section": "",
    "text": "Used when dealing with multivariable functions:\n\\[\n\\frac{\\partial f}{\\partial x}, \\quad \\frac{\\partial f}{\\partial y}\n\\]\nExample: For \\(f(x,y) = x^2y + 3y^2\\): - \\(\\frac{\\partial f}{\\partial x} = 2xy\\) - \\(\\frac{\\partial f}{\\partial y} = x^2 + 6y\\)\nML relevance: Computing gradients for each parameter in neural networks"
  },
  {
    "objectID": "calculus.html#gradient-vector",
    "href": "calculus.html#gradient-vector",
    "title": "Calculus",
    "section": "",
    "text": "The gradient is a vector of all partial derivatives:\n\\[\n\\nabla f = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_n} \\end{bmatrix}\n\\]\nProperties:\n\nPoints in direction of steepest ascent\nMagnitude indicates steepness\nPerpendicular to level curves/surfaces\n\nML relevance: Gradient descent uses \\(-\\nabla f\\) to minimize loss functions"
  },
  {
    "objectID": "calculus.html#jacobian-matrix",
    "href": "calculus.html#jacobian-matrix",
    "title": "Calculus",
    "section": "",
    "text": "For a vector-valued function \\(\\mathbf{f}: \\mathbb{R}^n \\to \\mathbb{R}^m\\), the Jacobian is:\n\\[\nJ_{ij} = \\frac{\\partial f_i}{\\partial x_j}\n\\]\nDimensions: \\(m \\times n\\) matrix\nExample: For \\(\\mathbf{f}(x,y) = [x^2y, xy^2]\\):\n\\[\nJ = \\begin{bmatrix}\n2xy & x^2 \\\\\ny^2 & 2xy\n\\end{bmatrix}\n\\]\nML relevance:\n\nBackpropagation through layers\nComputing derivatives of vector outputs"
  },
  {
    "objectID": "calculus.html#hessian-matrix",
    "href": "calculus.html#hessian-matrix",
    "title": "Calculus",
    "section": "",
    "text": "Second-order partial derivatives:\n\\[\nH_{ij} = \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}\n\\]\nProperties:\n\nSymmetric matrix (if \\(f\\) is twice differentiable)\nDiagonal elements: \\(\\frac{\\partial^2 f}{\\partial x_i^2}\\)\nOff-diagonal: mixed partials \\(\\frac{\\partial^2 f}{\\partial x_i \\partial x_j}\\)\n\nUsed in curvature analysis and second-order optimization.\nML relevance:\n\nNewton’s method optimization\nAnalyzing convergence properties\nIdentifying saddle points vs local minima"
  },
  {
    "objectID": "calculus.html#chain-rule-multivariate",
    "href": "calculus.html#chain-rule-multivariate",
    "title": "Calculus",
    "section": "",
    "text": "For composed functions \\(f(g(x))\\):\n\\[\n\\frac{\\partial f}{\\partial x_i} = \\sum_j \\frac{\\partial f}{\\partial g_j} \\cdot \\frac{\\partial g_j}{\\partial x_i}\n\\]\nMatrix form:\n\\[\n\\frac{\\partial f}{\\partial \\mathbf{x}} = \\frac{\\partial f}{\\partial \\mathbf{g}} \\cdot \\frac{\\partial \\mathbf{g}}{\\partial \\mathbf{x}}\n\\]\nML relevance: Foundation of backpropagation algorithm"
  },
  {
    "objectID": "calculus.html#directional-derivative",
    "href": "calculus.html#directional-derivative",
    "title": "Calculus",
    "section": "",
    "text": "Rate of change of \\(f\\) in direction of unit vector \\(\\mathbf{u}\\):\n\\[\nD_\\mathbf{u} f = \\nabla f \\cdot \\mathbf{u}\n\\]\nProperties:\n\nMaximum when \\(\\mathbf{u}\\) aligned with \\(\\nabla f\\)\nZero when \\(\\mathbf{u}\\) perpendicular to \\(\\nabla f\\)\n\nML relevance: Understanding optimization landscapes"
  },
  {
    "objectID": "calculus.html#taylor-series-expansion",
    "href": "calculus.html#taylor-series-expansion",
    "title": "Calculus",
    "section": "",
    "text": "Approximating functions using derivatives:\n\\[\nf(x + \\Delta x) \\approx f(x) + f'(x)\\Delta x + \\frac{1}{2}f''(x)\\Delta x^2 + \\cdots\n\\]\nFirst-order (linear) approximation:\n\\[\nf(x + \\Delta x) \\approx f(x) + f'(x)\\Delta x\n\\]\nSecond-order (quadratic) approximation:\n\\[\nf(x + \\Delta x) \\approx f(x) + f'(x)\\Delta x + \\frac{1}{2}f''(x)\\Delta x^2\n\\]\nMultivariate Taylor expansion:\n\\[\nf(\\mathbf{x} + \\mathbf{\\Delta x}) \\approx f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^T \\mathbf{\\Delta x} + \\frac{1}{2}\\mathbf{\\Delta x}^T H \\mathbf{\\Delta x}\n\\]\nML relevance:\n\nLocal approximations in optimization\nNewton’s method derivation\nTrust region methods"
  },
  {
    "objectID": "calculus.html#integration",
    "href": "calculus.html#integration",
    "title": "Calculus",
    "section": "",
    "text": "Area under a curve:\n\\[\n\\int_a^b f(x) \\, dx\n\\]\nCommon integrals:\n\n\\(\\int x^n \\, dx = \\frac{x^{n+1}}{n+1} + C\\) (for \\(n \\neq -1\\))\n\\(\\int e^x \\, dx = e^x + C\\)\n\\(\\int \\frac{1}{x} \\, dx = \\ln|x| + C\\)\n\\(\\int \\sin x \\, dx = -\\cos x + C\\)\n\\(\\int \\cos x \\, dx = \\sin x + C\\)\n\nML relevance:\n\nComputing expectations in probability\nNormalizing probability distributions\nDeriving closed-form solutions"
  },
  {
    "objectID": "calculus.html#integration-by-parts",
    "href": "calculus.html#integration-by-parts",
    "title": "Calculus",
    "section": "",
    "text": "A useful transformation:\n\\[\n\\int u \\, dv = uv - \\int v \\, du\n\\]\nML relevance:\n\nDeriving variational bounds\nEvidence lower bound (ELBO) derivations"
  },
  {
    "objectID": "calculus.html#fundamental-theorem-of-calculus",
    "href": "calculus.html#fundamental-theorem-of-calculus",
    "title": "Calculus",
    "section": "",
    "text": "Links differentiation and integration:\n\\[\n\\frac{d}{dx} \\int_a^x f(t) \\, dt = f(x)\n\\]\n\\[\n\\int_a^b f'(x) \\, dx = f(b) - f(a)\n\\]"
  },
  {
    "objectID": "calculus.html#optimization-conditions",
    "href": "calculus.html#optimization-conditions",
    "title": "Calculus",
    "section": "",
    "text": "At a local minimum/maximum \\(x^*\\):\n\\[\n\\nabla f(x^*) = \\mathbf{0}\n\\]\nCritical points: Where gradient vanishes\n\n\n\nFor a local minimum at \\(x^*\\):\n\n\\(\\nabla f(x^*) = \\mathbf{0}\\) (first-order condition)\nHessian \\(H(x^*)\\) is positive definite (all eigenvalues &gt; 0)\n\nFor a local maximum: - Hessian \\(H(x^*)\\) is negative definite (all eigenvalues &lt; 0)\nSaddle point: Hessian has both positive and negative eigenvalues\nML relevance: Analyzing loss function landscapes"
  },
  {
    "objectID": "calculus.html#convexity",
    "href": "calculus.html#convexity",
    "title": "Calculus",
    "section": "",
    "text": "A function \\(f\\) is convex if:\n\\[\nf(\\lambda x + (1-\\lambda)y) \\leq \\lambda f(x) + (1-\\lambda)f(y)\n\\]\nfor all \\(x, y\\) and \\(\\lambda \\in [0,1]\\)\nEquivalent conditions:\n\n\\(f''(x) \\geq 0\\) (for univariate functions)\nHessian \\(H\\) is positive semi-definite (for multivariate)\nAny local minimum is a global minimum\n\nStrictly convex: Use strict inequalities (\\(&gt;\\) instead of \\(\\geq\\))\nML relevance:\n\nGuarantees for optimization convergence\nLinear regression, logistic regression are convex\nNeural networks are generally non-convex"
  },
  {
    "objectID": "calculus.html#lagrange-multipliers",
    "href": "calculus.html#lagrange-multipliers",
    "title": "Calculus",
    "section": "",
    "text": "For constrained optimization:\n\\[\n\\min f(\\mathbf{x}) \\quad \\text{subject to} \\quad g(\\mathbf{x}) = 0\n\\]\nMethod: Solve:\n\\[\n\\nabla f(\\mathbf{x}) = \\lambda \\nabla g(\\mathbf{x})\n\\]\nand \\(g(\\mathbf{x}) = 0\\)\nLagrangian:\n\\[\n\\mathcal{L}(\\mathbf{x}, \\lambda) = f(\\mathbf{x}) - \\lambda g(\\mathbf{x})\n\\]\nML relevance:\n\nSupport Vector Machines (SVM)\nConstrained optimization problems\nDual formulations"
  },
  {
    "objectID": "calculus.html#lhôpitals-rule",
    "href": "calculus.html#lhôpitals-rule",
    "title": "Calculus",
    "section": "",
    "text": "For indeterminate forms \\(\\frac{0}{0}\\) or \\(\\frac{\\infty}{\\infty}\\):\n\\[\n\\lim_{x \\to c} \\frac{f(x)}{g(x)} = \\lim_{x \\to c} \\frac{f'(x)}{g'(x)}\n\\]\nML relevance: Analyzing limiting behavior of loss functions"
  },
  {
    "objectID": "calculus.html#common-activation-function-derivatives",
    "href": "calculus.html#common-activation-function-derivatives",
    "title": "Calculus",
    "section": "",
    "text": "Sigmoid: \\(\\sigma(x) = \\frac{1}{1 + e^{-x}}\\)\n\\[\n\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))\n\\]\nTanh: \\(\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\\)\n\\[\n\\tanh'(x) = 1 - \\tanh^2(x)\n\\]\nReLU: \\(\\text{ReLU}(x) = \\max(0, x)\\)\n\\[\n\\text{ReLU}'(x) = \\begin{cases} 1 & \\text{if } x &gt; 0 \\\\ 0 & \\text{if } x &lt; 0 \\\\ \\text{undefined} & \\text{if } x = 0 \\end{cases}\n\\]\nSoftmax: For vector \\(\\mathbf{z}\\), \\(\\text{softmax}(\\mathbf{z})_i = \\frac{e^{z_i}}{\\sum_j e^{z_j}}\\)\n\\[\n\\frac{\\partial \\text{softmax}(\\mathbf{z})_i}{\\partial z_j} = \\text{softmax}(\\mathbf{z})_i (\\delta_{ij} - \\text{softmax}(\\mathbf{z})_j)\n\\]\nwhere \\(\\delta_{ij}\\) is the Kronecker delta"
  },
  {
    "objectID": "calculus.html#gradient-descent",
    "href": "calculus.html#gradient-descent",
    "title": "Calculus",
    "section": "2.1 Gradient Descent",
    "text": "2.1 Gradient Descent\nUpdate rule:\n\\[\n\\mathbf{x}_{t+1} = \\mathbf{x}_t - \\alpha \\nabla f(\\mathbf{x}_t)\n\\]\nwhere \\(\\alpha\\) is the learning rate\nVariants: - Stochastic Gradient Descent (SGD): Use gradient from single sample - Mini-batch GD: Use gradient from small batch - Momentum: \\(\\mathbf{v}_{t+1} = \\beta \\mathbf{v}_t + \\nabla f(\\mathbf{x}_t)\\)"
  },
  {
    "objectID": "calculus.html#backpropagation",
    "href": "calculus.html#backpropagation",
    "title": "Calculus",
    "section": "2.2 Backpropagation",
    "text": "2.2 Backpropagation\nChain rule application:\nFor loss \\(L\\) and layer outputs \\(\\mathbf{z}^{(l)}\\):\n\\[\n\\frac{\\partial L}{\\partial \\mathbf{w}^{(l)}} = \\frac{\\partial L}{\\partial \\mathbf{z}^{(l)}} \\cdot \\frac{\\partial \\mathbf{z}^{(l)}}{\\partial \\mathbf{w}^{(l)}}\n\\]\nRecursive gradient flow:\n\\[\n\\frac{\\partial L}{\\partial \\mathbf{z}^{(l)}} = \\frac{\\partial L}{\\partial \\mathbf{z}^{(l+1)}} \\cdot \\frac{\\partial \\mathbf{z}^{(l+1)}}{\\partial \\mathbf{z}^{(l)}}\n\\]"
  },
  {
    "objectID": "calculus.html#loss-functions-and-their-derivatives",
    "href": "calculus.html#loss-functions-and-their-derivatives",
    "title": "Calculus",
    "section": "2.3 Loss Functions and Their Derivatives",
    "text": "2.3 Loss Functions and Their Derivatives\nMean Squared Error (MSE):\n\\[\nL = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n\\]\n\\[\n\\frac{\\partial L}{\\partial \\hat{y}_i} = -\\frac{2}{n}(y_i - \\hat{y}_i)\n\\]\nCross-Entropy Loss (binary):\n\\[\nL = -\\frac{1}{n}\\sum_{i=1}^n [y_i \\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i)]\n\\]\n\\[\n\\frac{\\partial L}{\\partial \\hat{y}_i} = -\\frac{1}{n}\\left[\\frac{y_i}{\\hat{y}_i} - \\frac{1-y_i}{1-\\hat{y}_i}\\right]\n\\]\nCategorical Cross-Entropy (with softmax):\n\\[\nL = -\\sum_{i=1}^n y_i \\log(\\hat{y}_i)\n\\]\nCombined softmax + cross-entropy derivative simplifies to:\n\\[\n\\frac{\\partial L}{\\partial z_i} = \\hat{y}_i - y_i\n\\]"
  },
  {
    "objectID": "calculus.html#regularization-terms",
    "href": "calculus.html#regularization-terms",
    "title": "Calculus",
    "section": "2.4 Regularization Terms",
    "text": "2.4 Regularization Terms\nL2 Regularization (Ridge):\n\\[\nR(\\mathbf{w}) = \\frac{\\lambda}{2}\\|\\mathbf{w}\\|_2^2 = \\frac{\\lambda}{2}\\sum_i w_i^2\n\\]\n\\[\n\\frac{\\partial R}{\\partial w_i} = \\lambda w_i\n\\]\nL1 Regularization (Lasso):\n\\[\nR(\\mathbf{w}) = \\lambda\\|\\mathbf{w}\\|_1 = \\lambda\\sum_i |w_i|\n\\]\n\\[\n\\frac{\\partial R}{\\partial w_i} = \\lambda \\cdot \\text{sign}(w_i)\n\\]"
  },
  {
    "objectID": "calculus.html#quick-reference-table",
    "href": "calculus.html#quick-reference-table",
    "title": "Calculus",
    "section": "2.5 Quick Reference Table",
    "text": "2.5 Quick Reference Table\n\n\n\n\n\n\n\n\nConcept\nFormula\nML Application\n\n\n\n\nGradient\n\\(\\nabla f\\)\nGradient descent direction\n\n\nJacobian\n\\(J_{ij} = \\frac{\\partial f_i}{\\partial x_j}\\)\nBackpropagation\n\n\nHessian\n\\(H_{ij} = \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}\\)\nSecond-order optimization\n\n\nChain Rule\n\\(\\frac{df}{dx} = \\frac{df}{dg} \\cdot \\frac{dg}{dx}\\)\nBackpropagation\n\n\nTaylor Expansion\n\\(f(x+\\Delta x) \\approx f(x) + f'(x)\\Delta x\\)\nLocal approximation\n\n\nConvexity\n\\(f''(x) \\geq 0\\)\nOptimization guarantees\n\n\nLagrange Multipliers\n\\(\\nabla f = \\lambda \\nabla g\\)\nConstrained optimization\n\n\nSigmoid Derivative\n\\(\\sigma'(x) = \\sigma(x)(1-\\sigma(x))\\)\nNeural network activations"
  },
  {
    "objectID": "linear-algebra.html",
    "href": "linear-algebra.html",
    "title": "Linear Algebra",
    "section": "",
    "text": "This section covers essential linear concepts that serve as the mathematical foundation for machine learning and deep learning.\n\n\n\nVector: A 1D array of numbers, often used to represent features.\nMatrix: A 2D array of numbers, common for representing datasets and weights.\n\nExample:\n\\[\n\\mathbf{x} = \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix}, \\quad\n\\mathbf{W} = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\n\\]\n\n\n\nUsed in the forward pass of neural networks:\n\\[\n\\mathbf{y} = \\mathbf{W}\\mathbf{x}\n\\]\n\n\n\nMeasures projection or similarity:\n\\[\n\\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i} a_i b_i\n\\]\nGeometric interpretation:\n\\[\n\\mathbf{a} \\cdot \\mathbf{b} = \\|\\mathbf{a}\\| \\|\\mathbf{b}\\| \\cos(\\theta)\n\\]\n\nUsed in cosine similarity for comparing vectors\n\n\n\n\n\nL2 norm (Euclidean length): \\[\n\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_i x_i^2}\n\\]\nL1 norm (Manhattan distance): \\[\n\\|\\mathbf{x}\\|_1 = \\sum_i |x_i|\n\\]\nFrobenius norm (for matrices): \\[\n\\|\\mathbf{A}\\|_F = \\sqrt{\\sum_{i,j} A_{ij}^2}\n\\]\n\nUsed to compute distances and regularization penalties.\n\n\n\nThe determinant of a square matrix \\(\\mathbf{A}\\) is a scalar value:\n\\[\n\\det(\\mathbf{A})\n\\]\n\n\\(\\det(\\mathbf{A}) = 0\\) means matrix is singular (non-invertible)\n\\(|\\det(\\mathbf{A})|\\) represents volume scaling factor\nUsed in linear transformations and invertibility checks\n\n\n\n\nThe trace of a square matrix is the sum of diagonal elements:\n\\[\n\\text{Tr}(\\mathbf{A}) = \\sum_i A_{ii}\n\\]\nProperties:\n\n\\(\\text{Tr}(\\mathbf{A} + \\mathbf{B}) = \\text{Tr}(\\mathbf{A}) + \\text{Tr}(\\mathbf{B})\\)\n\\(\\text{Tr}(\\mathbf{AB}) = \\text{Tr}(\\mathbf{BA})\\) (cyclic property)\n\\(\\text{Tr}(\\mathbf{A}) = \\sum_i \\lambda_i\\) (sum of eigenvalues)\n\n\n\n\nUsed in PCA (dimensionality reduction):\n\\[\n\\mathbf{A}\\mathbf{v} = \\lambda\\mathbf{v}\n\\]\nWhere:\n\n\\(\\mathbf{v}\\) is an eigenvector\n\\(\\lambda\\) is the corresponding eigenvalue\nCharacteristic equation: \\(\\det(\\mathbf{A} - \\lambda\\mathbf{I}) = 0\\)\n\nProperties:\n\n\\(\\text{Tr}(\\mathbf{A}) = \\sum_i \\lambda_i\\) (sum of eigenvalues)\n\\(\\det(\\mathbf{A}) = \\prod_i \\lambda_i\\) (product of eigenvalues)\n\n\n\n\nUsed in matrix factorization and recommender systems:\n\\[\n\\mathbf{A} = \\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^T\n\\]\nWhere:\n\n\\(\\mathbf{U}\\): left singular vectors (m × m orthogonal)\n\\(\\mathbf{\\Sigma}\\): diagonal matrix of singular values (m × n)\n\\(\\mathbf{V}\\): right singular vectors (n × n orthogonal)\n\nKey properties:\n\nAlways exists (unlike eigendecomposition)\nSingular values \\(\\sigma_i \\geq 0\\), ordered by magnitude\nLow-rank approximation: keep top \\(k\\) singular values\n\nApplications:\n\nDimensionality reduction (truncated SVD)\nImage compression\nCollaborative filtering\nPseudo-inverse calculation\n\n\n\n\n\nRank: The dimension of the column space (or row space):\n\\[\n\\text{rank}(\\mathbf{A}) = \\text{number of linearly independent columns}\n\\]\nProperties:\n\n\\(\\text{rank}(\\mathbf{A}) \\leq \\min(m, n)\\) for \\(m \\times n\\) matrix\nFull rank: \\(\\text{rank}(\\mathbf{A}) = \\min(m, n)\\)\nRank-deficient: \\(\\text{rank}(\\mathbf{A}) &lt; \\min(m, n)\\)\n\nInvertibility:\n\nSquare matrix \\(\\mathbf{A}\\) is invertible \\(\\Leftrightarrow\\) \\(\\text{rank}(\\mathbf{A}) = n\\)\n\\(\\mathbf{A}^{-1}\\) exists \\(\\Leftrightarrow\\) \\(\\det(\\mathbf{A}) \\neq 0\\)\nFor invertible \\(\\mathbf{A}\\): \\(\\mathbf{AA}^{-1} = \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}\\)\n\nML relevance: Feature redundancy, linear dependence in datasets\n\n\n\nOrthogonal vectors: \\(\\mathbf{x} \\perp \\mathbf{y}\\) if \\(\\mathbf{x}^T\\mathbf{y} = 0\\)\nOrthogonal matrix \\(\\mathbf{Q}\\):\n\n\\(\\mathbf{Q}^T\\mathbf{Q} = \\mathbf{QQ}^T = \\mathbf{I}\\)\nColumns are orthonormal\nPreserves lengths: \\(\\|\\mathbf{Qx}\\| = \\|\\mathbf{x}\\|\\)\n\nProjection matrix \\(\\mathbf{P}\\):\nProjection of \\(\\mathbf{b}\\) onto column space of \\(\\mathbf{A}\\):\n\\[\n\\mathbf{P} = \\mathbf{A}(\\mathbf{A}^T\\mathbf{A})^{-1}\\mathbf{A}^T\n\\]\nProperties:\n\n\\(\\mathbf{P}^2 = \\mathbf{P}\\) (idempotent)\n\\(\\mathbf{P}^T = \\mathbf{P}\\) (symmetric)\n\nQR Decomposition:\n\\[\n\\mathbf{A} = \\mathbf{QR}\n\\]\nWhere \\(\\mathbf{Q}\\) is orthogonal, \\(\\mathbf{R}\\) is upper triangular\nML relevance:\n\nLinear regression (least squares)\nGram-Schmidt orthogonalization\nSolving linear systems\n\n\n\n\nA symmetric matrix \\(\\mathbf{A}\\) is positive definite if:\n\\[\n\\mathbf{x}^T\\mathbf{A}\\mathbf{x} &gt; 0 \\text{ for all } \\mathbf{x} \\neq \\mathbf{0}\n\\]\nEquivalent conditions:\n\nAll eigenvalues \\(\\lambda_i &gt; 0\\)\nAll leading principal minors \\(&gt; 0\\)\n\\(\\mathbf{A} = \\mathbf{B}^T\\mathbf{B}\\) for some invertible \\(\\mathbf{B}\\)\n\nPositive semi-definite: \\(\\mathbf{x}^T\\mathbf{A}\\mathbf{x} \\geq 0\\) (allows zero eigenvalues)\nML relevance:\n\nCovariance matrices (always positive semi-definite)\nHessian matrices in optimization\nConvex optimization (positive definite Hessian → strict convexity)\nConvergence guarantees\n\n\n\n\nGradient (scalar function w.r.t vector):\n\\[\n\\nabla_\\mathbf{x} f(\\mathbf{x}) = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_n} \\end{bmatrix}\n\\]\nJacobian (vector function w.r.t vector):\n\\[\n\\mathbf{J} = \\begin{bmatrix} \\frac{\\partial f_i}{\\partial x_j} \\end{bmatrix} \\text{ (m × n matrix)}\n\\]\nCommon derivatives:\n\n\\(\\nabla_\\mathbf{x}(\\mathbf{a}^T\\mathbf{x}) = \\mathbf{a}\\)\n\\(\\nabla_\\mathbf{x}(\\mathbf{x}^T\\mathbf{A}\\mathbf{x}) = (\\mathbf{A} + \\mathbf{A}^T)\\mathbf{x}\\)\nFor symmetric \\(\\mathbf{A}\\): \\(\\nabla_\\mathbf{x}(\\mathbf{x}^T\\mathbf{A}\\mathbf{x}) = 2\\mathbf{A}\\mathbf{x}\\)\n\\(\\nabla_\\mathbf{x}(\\|\\mathbf{x}\\|^2) = 2\\mathbf{x}\\)\n\nHessian (second derivatives):\n\\[\n\\mathbf{H} = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_i \\partial x_j} \\end{bmatrix}\n\\]\nML relevance:\n\nBackpropagation\nGradient descent optimization\nNeural network training\n\n\n\n\nFor symmetric matrix \\(\\mathbf{A}\\):\n\\[\n\\mathbf{A} = \\mathbf{Q}\\mathbf{\\Lambda}\\mathbf{Q}^T\n\\]\nWhere:\n\n\\(\\mathbf{Q}\\): orthogonal matrix of eigenvectors\n\\(\\mathbf{\\Lambda}\\): diagonal matrix of eigenvalues\n\\(\\mathbf{Q}^T = \\mathbf{Q}^{-1}\\)\n\nProperties:\n\nAll eigenvalues are real\nEigenvectors are orthogonal\nCan always be diagonalized\n\nML relevance:\n\nPrincipal Component Analysis (PCA)\nCovariance matrix diagonalization\nQuadratic forms"
  },
  {
    "objectID": "linear-algebra.html#vectors-and-matrices",
    "href": "linear-algebra.html#vectors-and-matrices",
    "title": "Linear Algebra",
    "section": "",
    "text": "Vector: A 1D array of numbers, often used to represent features.\nMatrix: A 2D array of numbers, common for representing datasets and weights.\n\nExample:\n\\[\n\\mathbf{x} = \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix}, \\quad\n\\mathbf{W} = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "linear-algebra.html#matrix-multiplication",
    "href": "linear-algebra.html#matrix-multiplication",
    "title": "Linear Algebra",
    "section": "",
    "text": "Used in the forward pass of neural networks:\n\\[\n\\mathbf{y} = \\mathbf{W}\\mathbf{x}\n\\]"
  },
  {
    "objectID": "linear-algebra.html#dot-product",
    "href": "linear-algebra.html#dot-product",
    "title": "Linear Algebra",
    "section": "",
    "text": "Measures projection or similarity:\n\\[\n\\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i} a_i b_i\n\\]\nGeometric interpretation:\n\\[\n\\mathbf{a} \\cdot \\mathbf{b} = \\|\\mathbf{a}\\| \\|\\mathbf{b}\\| \\cos(\\theta)\n\\]\n\nUsed in cosine similarity for comparing vectors"
  },
  {
    "objectID": "linear-algebra.html#norms-and-distances",
    "href": "linear-algebra.html#norms-and-distances",
    "title": "Linear Algebra",
    "section": "",
    "text": "L2 norm (Euclidean length): \\[\n\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_i x_i^2}\n\\]\nL1 norm (Manhattan distance): \\[\n\\|\\mathbf{x}\\|_1 = \\sum_i |x_i|\n\\]\nFrobenius norm (for matrices): \\[\n\\|\\mathbf{A}\\|_F = \\sqrt{\\sum_{i,j} A_{ij}^2}\n\\]\n\nUsed to compute distances and regularization penalties."
  },
  {
    "objectID": "linear-algebra.html#determinant",
    "href": "linear-algebra.html#determinant",
    "title": "Linear Algebra",
    "section": "",
    "text": "The determinant of a square matrix \\(\\mathbf{A}\\) is a scalar value:\n\\[\n\\det(\\mathbf{A})\n\\]\n\n\\(\\det(\\mathbf{A}) = 0\\) means matrix is singular (non-invertible)\n\\(|\\det(\\mathbf{A})|\\) represents volume scaling factor\nUsed in linear transformations and invertibility checks"
  },
  {
    "objectID": "linear-algebra.html#trace",
    "href": "linear-algebra.html#trace",
    "title": "Linear Algebra",
    "section": "",
    "text": "The trace of a square matrix is the sum of diagonal elements:\n\\[\n\\text{Tr}(\\mathbf{A}) = \\sum_i A_{ii}\n\\]\nProperties:\n\n\\(\\text{Tr}(\\mathbf{A} + \\mathbf{B}) = \\text{Tr}(\\mathbf{A}) + \\text{Tr}(\\mathbf{B})\\)\n\\(\\text{Tr}(\\mathbf{AB}) = \\text{Tr}(\\mathbf{BA})\\) (cyclic property)\n\\(\\text{Tr}(\\mathbf{A}) = \\sum_i \\lambda_i\\) (sum of eigenvalues)"
  },
  {
    "objectID": "linear-algebra.html#eigenvalues-and-eigenvectors",
    "href": "linear-algebra.html#eigenvalues-and-eigenvectors",
    "title": "Linear Algebra",
    "section": "",
    "text": "Used in PCA (dimensionality reduction):\n\\[\n\\mathbf{A}\\mathbf{v} = \\lambda\\mathbf{v}\n\\]\nWhere:\n\n\\(\\mathbf{v}\\) is an eigenvector\n\\(\\lambda\\) is the corresponding eigenvalue\nCharacteristic equation: \\(\\det(\\mathbf{A} - \\lambda\\mathbf{I}) = 0\\)\n\nProperties:\n\n\\(\\text{Tr}(\\mathbf{A}) = \\sum_i \\lambda_i\\) (sum of eigenvalues)\n\\(\\det(\\mathbf{A}) = \\prod_i \\lambda_i\\) (product of eigenvalues)"
  },
  {
    "objectID": "linear-algebra.html#singular-value-decomposition-svd",
    "href": "linear-algebra.html#singular-value-decomposition-svd",
    "title": "Linear Algebra",
    "section": "",
    "text": "Used in matrix factorization and recommender systems:\n\\[\n\\mathbf{A} = \\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^T\n\\]\nWhere:\n\n\\(\\mathbf{U}\\): left singular vectors (m × m orthogonal)\n\\(\\mathbf{\\Sigma}\\): diagonal matrix of singular values (m × n)\n\\(\\mathbf{V}\\): right singular vectors (n × n orthogonal)\n\nKey properties:\n\nAlways exists (unlike eigendecomposition)\nSingular values \\(\\sigma_i \\geq 0\\), ordered by magnitude\nLow-rank approximation: keep top \\(k\\) singular values\n\nApplications:\n\nDimensionality reduction (truncated SVD)\nImage compression\nCollaborative filtering\nPseudo-inverse calculation"
  },
  {
    "objectID": "linear-algebra.html#matrix-rank-and-invertibility",
    "href": "linear-algebra.html#matrix-rank-and-invertibility",
    "title": "Linear Algebra",
    "section": "",
    "text": "Rank: The dimension of the column space (or row space):\n\\[\n\\text{rank}(\\mathbf{A}) = \\text{number of linearly independent columns}\n\\]\nProperties:\n\n\\(\\text{rank}(\\mathbf{A}) \\leq \\min(m, n)\\) for \\(m \\times n\\) matrix\nFull rank: \\(\\text{rank}(\\mathbf{A}) = \\min(m, n)\\)\nRank-deficient: \\(\\text{rank}(\\mathbf{A}) &lt; \\min(m, n)\\)\n\nInvertibility:\n\nSquare matrix \\(\\mathbf{A}\\) is invertible \\(\\Leftrightarrow\\) \\(\\text{rank}(\\mathbf{A}) = n\\)\n\\(\\mathbf{A}^{-1}\\) exists \\(\\Leftrightarrow\\) \\(\\det(\\mathbf{A}) \\neq 0\\)\nFor invertible \\(\\mathbf{A}\\): \\(\\mathbf{AA}^{-1} = \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}\\)\n\nML relevance: Feature redundancy, linear dependence in datasets"
  },
  {
    "objectID": "linear-algebra.html#orthogonality-and-projections",
    "href": "linear-algebra.html#orthogonality-and-projections",
    "title": "Linear Algebra",
    "section": "",
    "text": "Orthogonal vectors: \\(\\mathbf{x} \\perp \\mathbf{y}\\) if \\(\\mathbf{x}^T\\mathbf{y} = 0\\)\nOrthogonal matrix \\(\\mathbf{Q}\\):\n\n\\(\\mathbf{Q}^T\\mathbf{Q} = \\mathbf{QQ}^T = \\mathbf{I}\\)\nColumns are orthonormal\nPreserves lengths: \\(\\|\\mathbf{Qx}\\| = \\|\\mathbf{x}\\|\\)\n\nProjection matrix \\(\\mathbf{P}\\):\nProjection of \\(\\mathbf{b}\\) onto column space of \\(\\mathbf{A}\\):\n\\[\n\\mathbf{P} = \\mathbf{A}(\\mathbf{A}^T\\mathbf{A})^{-1}\\mathbf{A}^T\n\\]\nProperties:\n\n\\(\\mathbf{P}^2 = \\mathbf{P}\\) (idempotent)\n\\(\\mathbf{P}^T = \\mathbf{P}\\) (symmetric)\n\nQR Decomposition:\n\\[\n\\mathbf{A} = \\mathbf{QR}\n\\]\nWhere \\(\\mathbf{Q}\\) is orthogonal, \\(\\mathbf{R}\\) is upper triangular\nML relevance:\n\nLinear regression (least squares)\nGram-Schmidt orthogonalization\nSolving linear systems"
  },
  {
    "objectID": "linear-algebra.html#positive-definite-matrices",
    "href": "linear-algebra.html#positive-definite-matrices",
    "title": "Linear Algebra",
    "section": "",
    "text": "A symmetric matrix \\(\\mathbf{A}\\) is positive definite if:\n\\[\n\\mathbf{x}^T\\mathbf{A}\\mathbf{x} &gt; 0 \\text{ for all } \\mathbf{x} \\neq \\mathbf{0}\n\\]\nEquivalent conditions:\n\nAll eigenvalues \\(\\lambda_i &gt; 0\\)\nAll leading principal minors \\(&gt; 0\\)\n\\(\\mathbf{A} = \\mathbf{B}^T\\mathbf{B}\\) for some invertible \\(\\mathbf{B}\\)\n\nPositive semi-definite: \\(\\mathbf{x}^T\\mathbf{A}\\mathbf{x} \\geq 0\\) (allows zero eigenvalues)\nML relevance:\n\nCovariance matrices (always positive semi-definite)\nHessian matrices in optimization\nConvex optimization (positive definite Hessian → strict convexity)\nConvergence guarantees"
  },
  {
    "objectID": "linear-algebra.html#matrix-calculus-basics",
    "href": "linear-algebra.html#matrix-calculus-basics",
    "title": "Linear Algebra",
    "section": "",
    "text": "Gradient (scalar function w.r.t vector):\n\\[\n\\nabla_\\mathbf{x} f(\\mathbf{x}) = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_n} \\end{bmatrix}\n\\]\nJacobian (vector function w.r.t vector):\n\\[\n\\mathbf{J} = \\begin{bmatrix} \\frac{\\partial f_i}{\\partial x_j} \\end{bmatrix} \\text{ (m × n matrix)}\n\\]\nCommon derivatives:\n\n\\(\\nabla_\\mathbf{x}(\\mathbf{a}^T\\mathbf{x}) = \\mathbf{a}\\)\n\\(\\nabla_\\mathbf{x}(\\mathbf{x}^T\\mathbf{A}\\mathbf{x}) = (\\mathbf{A} + \\mathbf{A}^T)\\mathbf{x}\\)\nFor symmetric \\(\\mathbf{A}\\): \\(\\nabla_\\mathbf{x}(\\mathbf{x}^T\\mathbf{A}\\mathbf{x}) = 2\\mathbf{A}\\mathbf{x}\\)\n\\(\\nabla_\\mathbf{x}(\\|\\mathbf{x}\\|^2) = 2\\mathbf{x}\\)\n\nHessian (second derivatives):\n\\[\n\\mathbf{H} = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_i \\partial x_j} \\end{bmatrix}\n\\]\nML relevance:\n\nBackpropagation\nGradient descent optimization\nNeural network training"
  },
  {
    "objectID": "linear-algebra.html#spectral-theorem",
    "href": "linear-algebra.html#spectral-theorem",
    "title": "Linear Algebra",
    "section": "",
    "text": "For symmetric matrix \\(\\mathbf{A}\\):\n\\[\n\\mathbf{A} = \\mathbf{Q}\\mathbf{\\Lambda}\\mathbf{Q}^T\n\\]\nWhere:\n\n\\(\\mathbf{Q}\\): orthogonal matrix of eigenvectors\n\\(\\mathbf{\\Lambda}\\): diagonal matrix of eigenvalues\n\\(\\mathbf{Q}^T = \\mathbf{Q}^{-1}\\)\n\nProperties:\n\nAll eigenvalues are real\nEigenvectors are orthogonal\nCan always be diagonalized\n\nML relevance:\n\nPrincipal Component Analysis (PCA)\nCovariance matrix diagonalization\nQuadratic forms"
  }
]
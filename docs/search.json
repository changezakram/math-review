[
  {
    "objectID": "probability.html",
    "href": "probability.html",
    "title": "Probability",
    "section": "",
    "text": "Probability is central to machine learning — particularly in generative models, classification, and Bayesian methods.\n\n\n\n\n\nDiscrete: Bernoulli, Binomial, Poisson, Categorical\nContinuous: Normal (Gaussian), Exponential, Uniform, Beta\n\nNotation:\n\n\\(P(X = x)\\) (discrete): Probability mass function (PMF)\n\\(f_X(x)\\) (continuous): Probability density function (PDF)\n\\(F_X(x) = P(X \\leq x)\\): Cumulative distribution function (CDF)\n\n\n\n\n\n\n\nThe average or mean value:\nDiscrete: \\[\n\\mathbb{E}[X] = \\sum_x x P(X = x)\n\\]\nContinuous: \\[\n\\mathbb{E}[X] = \\int x f_X(x) \\, dx\n\\]\nProperties:\n\nLinearity: \\(\\mathbb{E}[aX + bY] = a\\mathbb{E}[X] + b\\mathbb{E}[Y]\\)\n\\(\\mathbb{E}[c] = c\\) for constant \\(c\\)\n\\(\\mathbb{E}[g(X)] = \\sum_x g(x)P(X=x)\\) or \\(\\int g(x)f_X(x)dx\\)\n\n\n\n\nMeasure of spread around the mean:\n\\[\n\\text{Var}(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2] = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2\n\\]\nProperties:\n\n\\(\\text{Var}(aX + b) = a^2 \\text{Var}(X)\\)\nFor independent \\(X, Y\\): \\(\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y)\\)\n\nStandard Deviation: \\(\\sigma = \\sqrt{\\text{Var}(X)}\\)\n\n\n\nCovariance: Measures linear relationship between two variables\n\\[\n\\text{Cov}(X, Y) = \\mathbb{E}[(X - \\mathbb{E}[X])(Y - \\mathbb{E}[Y])] = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y]\n\\]\nProperties:\n\n\\(\\text{Cov}(X, X) = \\text{Var}(X)\\)\n\\(\\text{Cov}(X, Y) = \\text{Cov}(Y, X)\\) (symmetric)\nIf \\(X, Y\\) independent: \\(\\text{Cov}(X, Y) = 0\\) (converse not always true)\n\nCorrelation coefficient:\n\\[\n\\rho(X, Y) = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y}\n\\]\nwhere \\(-1 \\leq \\rho \\leq 1\\)\nML relevance: Feature correlation analysis, covariance matrices in PCA\n\n\n\n\n\n\nProbability of multiple events occurring together:\n\\[\nP(X = x, Y = y) \\quad \\text{or} \\quad f_{X,Y}(x, y)\n\\]\n\n\n\nProbability of one variable, ignoring others:\nDiscrete: \\[\nP(X = x) = \\sum_y P(X = x, Y = y)\n\\]\nContinuous: \\[\nf_X(x) = \\int f_{X,Y}(x, y) \\, dy\n\\]\n\n\n\nProbability of \\(A\\) given \\(B\\) has occurred:\n\\[\nP(A \\mid B) = \\frac{P(A \\cap B)}{P(B)} = \\frac{P(A, B)}{P(B)}\n\\]\nChain rule: \\[\nP(A, B) = P(A \\mid B) P(B) = P(B \\mid A) P(A)\n\\]\nGeneral chain rule (for multiple variables): \\[\nP(X_1, X_2, \\ldots, X_n) = P(X_1) P(X_2 \\mid X_1) P(X_3 \\mid X_1, X_2) \\cdots P(X_n \\mid X_1, \\ldots, X_{n-1})\n\\]\n\n\n\n\n\n\n\\(X\\) and \\(Y\\) are independent if:\n\\[\nP(X, Y) = P(X) P(Y)\n\\]\nEquivalent conditions:\n\n\\(P(X \\mid Y) = P(X)\\)\n\\(P(Y \\mid X) = P(Y)\\)\n\\(\\mathbb{E}[XY] = \\mathbb{E}[X]\\mathbb{E}[Y]\\)\n\n\n\n\n\\(X\\) and \\(Y\\) are conditionally independent given \\(Z\\) if:\n\\[\nP(X, Y \\mid Z) = P(X \\mid Z) P(Y \\mid Z)\n\\]\nNotation: \\(X \\perp Y \\mid Z\\)\nML relevance:\n\nNaive Bayes assumes features are conditionally independent given class\nGraphical models encode conditional independence\n\n\n\n\n\nBayes’ Theorem lets us update what we believe about a situation after seeing new evidence.\nThe formula:\n\\[\nP(A \\mid B) = \\frac{P(B \\mid A) \\cdot P(A)}{P(B)}\n\\]\nComponents:\n\n\\(P(A \\mid B)\\): Posterior — probability of \\(A\\) given that \\(B\\) happened\n\\(P(B \\mid A)\\): Likelihood — how likely is \\(B\\) if \\(A\\) is true\n\\(P(A)\\): Prior — our initial belief about \\(A\\)\n\\(P(B)\\): Evidence — total probability of \\(B\\) happening under all possibilities\n\nExpanded form (Law of Total Probability):\n\\[\nP(A \\mid B) = \\frac{P(B \\mid A) P(A)}{P(B \\mid A)P(A) + P(B \\mid A^c)P(A^c)}\n\\]\n\n\nBayes’ Theorem is about updating your belief:\n\nStart with your prior belief \\(P(A)\\)\nThen observe new evidence \\((B)\\)\nUpdate your belief using how likely that evidence is under \\(A\\) (\\(P(B \\mid A)\\))\n\n\n\n\nSuppose a disease affects 1% of the population.\nYou take a test that is: - \\(P(\\text{Positive} \\mid \\text{Disease}) = 0.99\\) (true positive rate) - \\(P(\\text{Positive} \\mid \\text{No Disease}) = 0.05\\) (false positive rate)\nYou test positive. What is the chance you actually have the disease?\nWe want to compute:\n\\[\nP(\\text{Disease} \\mid \\text{Positive}) = \\frac{P(\\text{Positive} \\mid \\text{Disease}) \\cdot P(\\text{Disease})}{P(\\text{Positive})}\n\\]\nLet’s plug in values: - \\(P(\\text{Disease}) = 0.01\\) - \\(P(\\text{No Disease}) = 0.99\\) - \\(P(\\text{Positive}) = 0.99 \\cdot 0.01 + 0.05 \\cdot 0.99 = 0.0594\\)\nSo:\n\\[\nP(\\text{Disease} \\mid \\text{Positive}) = \\frac{0.99 \\cdot 0.01}{0.0594} \\approx 0.167\n\\]\nSurprising result: Even after a positive test, the chance of having the disease is only about 16.7%, because false positives are more common than true positives.\n\n\n\nBayes’ Theorem is widely used in:\n\nMedical diagnosis\nSpam detection\nProbabilistic machine learning (e.g., Naive Bayes classifiers)\nUpdating beliefs in AI models\nA/B testing and experimental design\n\n\n\n\nIn many machine learning applications, the denominator \\(P(B)\\) is the same for all outcomes and can be ignored:\n\\[\nP(A \\mid B) \\propto P(B \\mid A) \\cdot P(A)\n\\]\nThis version is often used for ranking outcomes instead of calculating exact probabilities.\nML relevance: Naive Bayes classification, Bayesian inference\n\n\n\n\nFor a partition of the sample space \\(\\{A_1, A_2, \\ldots, A_n\\}\\):\n\\[\nP(B) = \\sum_{i=1}^n P(B \\mid A_i) P(A_i)\n\\]\nContinuous version:\n\\[\nP(B) = \\int P(B \\mid A = a) P(A = a) \\, da\n\\]\nML relevance: Computing marginal probabilities, evidence in Bayesian inference\n\n\n\nProbability distributions describe how values of a random variable are distributed.\n\n\n\n\nSingle binary trial (success/failure):\n\\[\nP(X = 1) = p, \\quad P(X = 0) = 1 - p\n\\]\n\n\\(\\mathbb{E}[X] = p\\)\n\\(\\text{Var}(X) = p(1-p)\\)\n\nML relevance: Binary classification outputs\n\n\n\nNumber of successes in \\(n\\) Bernoulli trials:\n\\[\nP(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}\n\\]\n\n\\(\\mathbb{E}[X] = np\\)\n\\(\\text{Var}(X) = np(1-p)\\)\n\n\n\n\nGeneralization of Bernoulli to \\(k\\) categories:\n\\[\nP(X = i) = p_i, \\quad \\sum_{i=1}^k p_i = 1\n\\]\nML relevance: Multi-class classification\n\n\n\nNumber of events in fixed interval:\n\\[\nP(X = k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\n\\]\n\n\\(\\mathbb{E}[X] = \\lambda\\)\n\\(\\text{Var}(X) = \\lambda\\)\n\nML relevance: Count data, rare events\n\n\n\n\n\n\nEqual probability over interval \\([a, b]\\):\n\\[\nf(x) = \\frac{1}{b - a}, \\quad a \\leq x \\leq b\n\\]\n\n\\(\\mathbb{E}[X] = \\frac{a + b}{2}\\)\n\\(\\text{Var}(X) = \\frac{(b-a)^2}{12}\\)\n\nML relevance: Random initialization, data augmentation\n\n\n\nTime between events in Poisson process:\n\\[\nf(x) = \\lambda e^{-\\lambda x}, \\quad x \\geq 0\n\\]\n\n\\(\\mathbb{E}[X] = \\frac{1}{\\lambda}\\)\n\\(\\text{Var}(X) = \\frac{1}{\\lambda^2}\\)\n\n\n\n\nPDF of normal distribution in 1D:\n\\[\nf(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\n\\]\nNotation: \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\)\nParameters: - \\(\\mu\\): mean - \\(\\sigma^2\\): variance - \\(\\sigma\\): standard deviation\nProperties:\n\nSymmetric around \\(\\mu\\)\n68-95-99.7 rule: ~68% within 1σ, ~95% within 2σ, ~99.7% within 3σ\nSum of independent Gaussians is Gaussian\nCentral Limit Theorem: sum of many i.i.d. variables → Gaussian\n\nStandard Normal: \\(\\mathcal{N}(0, 1)\\)\n\\[\nZ = \\frac{X - \\mu}{\\sigma} \\sim \\mathcal{N}(0, 1)\n\\]\n\n\n\nThe normal (Gaussian) distribution can be extended to multiple variables — for example, when \\(\\mathbf{x}\\) is a vector instead of just a number.\nWhen \\(\\mathbf{x}\\) is a vector in \\(\\mathbb{R}^d\\) (i.e., a list of \\(d\\) values), the multivariate Gaussian looks like:\n\\[\n\\mathcal{N}(\\mathbf{x} \\mid \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\frac{1}{(2\\pi)^{d/2}|\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left(-\\frac{1}{2}(\\mathbf{x} - \\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1}(\\mathbf{x} - \\boldsymbol{\\mu})\\right)\n\\]\nWhat do all these symbols mean?\n\n\\(\\mathbf{x}\\): A \\(d\\)-dimensional vector (e.g., an image flattened into a 1D array)\n\\(\\boldsymbol{\\mu}\\): The mean vector (the “center” of the distribution)\n\\(\\boldsymbol{\\Sigma}\\): The \\(d \\times d\\) covariance matrix, which captures the spread and correlation of the variables\n\\(|\\boldsymbol{\\Sigma}|\\): The determinant of \\(\\boldsymbol{\\Sigma}\\), representing the volume scaling\n\\((2\\pi)^{d/2}\\): Comes from extending the 1D normalizing constant to \\(d\\) dimensions\n\\((\\mathbf{x} - \\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1}(\\mathbf{x} - \\boldsymbol{\\mu})\\): A generalized squared distance from \\(\\mathbf{x}\\) to the mean — see below!\n\nWhat’s the Mahalanobis Distance?\nThe term \\((\\mathbf{x} - \\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1}(\\mathbf{x} - \\boldsymbol{\\mu})\\) measures how far \\(\\mathbf{x}\\) is from the mean \\(\\boldsymbol{\\mu}\\), taking into account how spread out the data is in different directions. It’s like Euclidean distance, but adjusted for direction-dependent variance. The more variance in a direction, the less distance is penalized in that direction.\nSpecial Case: Spherical Gaussian\nIf all variables are independent and have equal variance \\(\\sigma^2\\), then \\(\\boldsymbol{\\Sigma} = \\sigma^2 \\mathbf{I}\\) and the formula simplifies to:\n\\[\n\\mathcal{N}(\\mathbf{x} \\mid \\boldsymbol{\\mu}, \\sigma^2 \\mathbf{I}) = \\frac{1}{(2\\pi\\sigma^2)^{d/2}} \\exp\\left(-\\frac{1}{2\\sigma^2}\\|\\mathbf{x} - \\boldsymbol{\\mu}\\|^2\\right)\n\\]\nThis looks more like the familiar 1D bell curve — but extended to \\(d\\) dimensions.\nML relevance:\n\nGaussian Mixture Models (GMM)\nLatent variable models (VAE)\nGaussian processes\nDiscriminant analysis\n\n\n\n\nProbability distribution over probabilities \\([0, 1]\\):\n\\[\nf(x) = \\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{B(\\alpha, \\beta)}, \\quad 0 \\leq x \\leq 1\n\\]\nwhere \\(B(\\alpha, \\beta)\\) is the beta function\nML relevance: Bayesian inference (conjugate prior for Bernoulli/Binomial)\n\n\n\nGeneralization of exponential distribution:\n\\[\nf(x) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} x^{\\alpha-1} e^{-\\beta x}, \\quad x \\geq 0\n\\]\nML relevance: Prior distributions in Bayesian models\n\n\n\n\n\nUsed to estimate parameters of models:\n\\[\n\\hat{\\theta}_{\\text{MLE}} = \\arg\\max_\\theta P(\\mathcal{D} \\mid \\theta)\n\\]\nEquivalently (using log-likelihood):\n\\[\n\\hat{\\theta}_{\\text{MLE}} = \\arg\\max_\\theta \\sum_{i=1}^n \\log P(x_i \\mid \\theta)\n\\]\nWhy log-likelihood?\n\nProducts become sums (easier to optimize)\nNumerically more stable\nSame maximum as likelihood\n\nExample: MLE for Gaussian\nGiven data \\(\\{x_1, \\ldots, x_n\\}\\) from \\(\\mathcal{N}(\\mu, \\sigma^2)\\):\n\\[\n\\hat{\\mu}_{\\text{MLE}} = \\frac{1}{n}\\sum_{i=1}^n x_i \\quad \\text{(sample mean)}\n\\]\n\\[\n\\hat{\\sigma}^2_{\\text{MLE}} = \\frac{1}{n}\\sum_{i=1}^n (x_i - \\hat{\\mu})^2 \\quad \\text{(sample variance)}\n\\]\nML relevance: Training probabilistic models, logistic regression\n\n\n\nIncorporates prior knowledge:\n\\[\n\\hat{\\theta}_{\\text{MAP}} = \\arg\\max_\\theta P(\\theta \\mid \\mathcal{D}) = \\arg\\max_\\theta P(\\mathcal{D} \\mid \\theta) P(\\theta)\n\\]\nUsing log:\n\\[\n\\hat{\\theta}_{\\text{MAP}} = \\arg\\max_\\theta \\left[\\log P(\\mathcal{D} \\mid \\theta) + \\log P(\\theta)\\right]\n\\]\nDifference from MLE: MAP includes prior \\(P(\\theta)\\)\nML relevance:\n\nRegularization (L2 = Gaussian prior, L1 = Laplace prior)\nBayesian neural networks\n\n\n\n\nMeasures how one distribution diverges from another:\n\\[\nD_{\\text{KL}}(P \\| Q) = \\sum_x P(x) \\log \\frac{P(x)}{Q(x)}\n\\]\nContinuous:\n\\[\nD_{\\text{KL}}(P \\| Q) = \\int P(x) \\log \\frac{P(x)}{Q(x)} \\, dx\n\\]\nProperties:\n\n\\(D_{\\text{KL}}(P \\| Q) \\geq 0\\) (Gibb’s inequality)\n\\(D_{\\text{KL}}(P \\| Q) = 0\\) iff \\(P = Q\\)\nNot symmetric: \\(D_{\\text{KL}}(P \\| Q) \\neq D_{\\text{KL}}(Q \\| P)\\)\nNot a true distance metric\n\nEquivalent form:\n\\[\nD_{\\text{KL}}(P \\| Q) = \\mathbb{E}_{x \\sim P}\\left[\\log \\frac{P(x)}{Q(x)}\\right] = \\mathbb{E}_{x \\sim P}[\\log P(x)] - \\mathbb{E}_{x \\sim P}[\\log Q(x)]\n\\]\n\\[\n= -H(P) - \\mathbb{E}_{x \\sim P}[\\log Q(x)]\n\\]\nwhere \\(H(P)\\) is the entropy of \\(P\\)\nML relevance:\n\nVariational inference\nTraining VAEs (ELBO)\nInformation-theoretic learning\nModel comparison\n\n\n\n\nMeasures expected log-likelihood under distribution \\(Q\\) when true distribution is \\(P\\):\n\\[\nH(P, Q) = -\\sum_x P(x) \\log Q(x)\n\\]\nContinuous:\n\\[\nH(P, Q) = -\\int P(x) \\log Q(x) \\, dx\n\\]\nRelation to KL divergence:\n\\[\nH(P, Q) = H(P) + D_{\\text{KL}}(P \\| Q)\n\\]\nwhere \\(H(P) = -\\sum_x P(x) \\log P(x)\\) is the entropy\nML relevance:\n\nCross-entropy loss in classification\nMinimizing cross-entropy ≡ minimizing KL divergence (since \\(H(P)\\) is constant)\n\n\n\n\nMeasures uncertainty or information content:\nShannon Entropy:\n\\[\nH(X) = -\\sum_x P(x) \\log P(x) = \\mathbb{E}[-\\log P(X)]\n\\]\nContinuous (Differential Entropy):\n\\[\nH(X) = -\\int f(x) \\log f(x) \\, dx\n\\]\nProperties:\n\n\\(H(X) \\geq 0\\)\nMaximum entropy for discrete uniform distribution\nHigher entropy = more uncertainty\n\nConditional Entropy:\n\\[\nH(Y \\mid X) = \\sum_x P(x) H(Y \\mid X = x)\n\\]\nML relevance:\n\nInformation gain in decision trees\nEntropy regularization\nInformation theory foundations\n\n\n\n\nMeasures dependence between variables:\n\\[\nI(X; Y) = D_{\\text{KL}}(P(X,Y) \\| P(X)P(Y))\n\\]\nEquivalent forms:\n\\[\nI(X; Y) = H(X) - H(X \\mid Y) = H(Y) - H(Y \\mid X)\n\\]\n\\[\nI(X; Y) = H(X) + H(Y) - H(X, Y)\n\\]\nProperties:\n\n\\(I(X; Y) \\geq 0\\)\n\\(I(X; Y) = 0\\) iff \\(X\\) and \\(Y\\) independent\nSymmetric: \\(I(X; Y) = I(Y; X)\\)\n\nML relevance:\n\nFeature selection\nInformation bottleneck theory\nVariational information maximization\n\n\n\n\nFor i.i.d. random variables \\(X_1, \\ldots, X_n\\) with mean \\(\\mu\\) and variance \\(\\sigma^2\\):\n\\[\n\\frac{\\bar{X}_n - \\mu}{\\sigma/\\sqrt{n}} \\xrightarrow{d} \\mathcal{N}(0, 1)\n\\]\nwhere \\(\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^n X_i\\)\nPractical form: For large \\(n\\):\n\\[\n\\bar{X}_n \\approx \\mathcal{N}\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\n\\]\nML relevance:\n\nJustifies Gaussian assumptions\nConfidence intervals\nBootstrap methods\n\n\n\n\n\n\nFor i.i.d. \\(X_1, \\ldots, X_n\\) with mean \\(\\mu\\):\n\\[\n\\bar{X}_n \\xrightarrow{P} \\mu\n\\]\n(Sample mean converges in probability to true mean)\n\n\n\n\\[\n\\bar{X}_n \\xrightarrow{\\text{a.s.}} \\mu\n\\]\n(Sample mean converges almost surely)\nML relevance:\n\nMonte Carlo methods\nJustifies empirical risk minimization\n\n\n\n\n\nFor convex function \\(f\\) and random variable \\(X\\):\n\\[\nf(\\mathbb{E}[X]) \\leq \\mathbb{E}[f(X)]\n\\]\nFor concave function: reverse inequality\nExample: \\(\\log\\) is concave, so:\n\\[\n\\log(\\mathbb{E}[X]) \\geq \\mathbb{E}[\\log(X)]\n\\]\nML relevance:\n\nDerives EM algorithm\nVariational inference bounds\nInformation theory inequalities"
  },
  {
    "objectID": "probability.html#random-variables-and-distributions",
    "href": "probability.html#random-variables-and-distributions",
    "title": "Probability",
    "section": "",
    "text": "Discrete: Bernoulli, Binomial, Poisson, Categorical\nContinuous: Normal (Gaussian), Exponential, Uniform, Beta\n\nNotation:\n\n\\(P(X = x)\\) (discrete): Probability mass function (PMF)\n\\(f_X(x)\\) (continuous): Probability density function (PDF)\n\\(F_X(x) = P(X \\leq x)\\): Cumulative distribution function (CDF)"
  },
  {
    "objectID": "probability.html#expectation-and-variance",
    "href": "probability.html#expectation-and-variance",
    "title": "Probability",
    "section": "",
    "text": "The average or mean value:\nDiscrete: \\[\n\\mathbb{E}[X] = \\sum_x x P(X = x)\n\\]\nContinuous: \\[\n\\mathbb{E}[X] = \\int x f_X(x) \\, dx\n\\]\nProperties:\n\nLinearity: \\(\\mathbb{E}[aX + bY] = a\\mathbb{E}[X] + b\\mathbb{E}[Y]\\)\n\\(\\mathbb{E}[c] = c\\) for constant \\(c\\)\n\\(\\mathbb{E}[g(X)] = \\sum_x g(x)P(X=x)\\) or \\(\\int g(x)f_X(x)dx\\)\n\n\n\n\nMeasure of spread around the mean:\n\\[\n\\text{Var}(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2] = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2\n\\]\nProperties:\n\n\\(\\text{Var}(aX + b) = a^2 \\text{Var}(X)\\)\nFor independent \\(X, Y\\): \\(\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y)\\)\n\nStandard Deviation: \\(\\sigma = \\sqrt{\\text{Var}(X)}\\)\n\n\n\nCovariance: Measures linear relationship between two variables\n\\[\n\\text{Cov}(X, Y) = \\mathbb{E}[(X - \\mathbb{E}[X])(Y - \\mathbb{E}[Y])] = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y]\n\\]\nProperties:\n\n\\(\\text{Cov}(X, X) = \\text{Var}(X)\\)\n\\(\\text{Cov}(X, Y) = \\text{Cov}(Y, X)\\) (symmetric)\nIf \\(X, Y\\) independent: \\(\\text{Cov}(X, Y) = 0\\) (converse not always true)\n\nCorrelation coefficient:\n\\[\n\\rho(X, Y) = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y}\n\\]\nwhere \\(-1 \\leq \\rho \\leq 1\\)\nML relevance: Feature correlation analysis, covariance matrices in PCA"
  },
  {
    "objectID": "probability.html#joint-marginal-and-conditional-probability",
    "href": "probability.html#joint-marginal-and-conditional-probability",
    "title": "Probability",
    "section": "",
    "text": "Probability of multiple events occurring together:\n\\[\nP(X = x, Y = y) \\quad \\text{or} \\quad f_{X,Y}(x, y)\n\\]\n\n\n\nProbability of one variable, ignoring others:\nDiscrete: \\[\nP(X = x) = \\sum_y P(X = x, Y = y)\n\\]\nContinuous: \\[\nf_X(x) = \\int f_{X,Y}(x, y) \\, dy\n\\]\n\n\n\nProbability of \\(A\\) given \\(B\\) has occurred:\n\\[\nP(A \\mid B) = \\frac{P(A \\cap B)}{P(B)} = \\frac{P(A, B)}{P(B)}\n\\]\nChain rule: \\[\nP(A, B) = P(A \\mid B) P(B) = P(B \\mid A) P(A)\n\\]\nGeneral chain rule (for multiple variables): \\[\nP(X_1, X_2, \\ldots, X_n) = P(X_1) P(X_2 \\mid X_1) P(X_3 \\mid X_1, X_2) \\cdots P(X_n \\mid X_1, \\ldots, X_{n-1})\n\\]"
  },
  {
    "objectID": "probability.html#independence",
    "href": "probability.html#independence",
    "title": "Probability",
    "section": "",
    "text": "\\(X\\) and \\(Y\\) are independent if:\n\\[\nP(X, Y) = P(X) P(Y)\n\\]\nEquivalent conditions:\n\n\\(P(X \\mid Y) = P(X)\\)\n\\(P(Y \\mid X) = P(Y)\\)\n\\(\\mathbb{E}[XY] = \\mathbb{E}[X]\\mathbb{E}[Y]\\)\n\n\n\n\n\\(X\\) and \\(Y\\) are conditionally independent given \\(Z\\) if:\n\\[\nP(X, Y \\mid Z) = P(X \\mid Z) P(Y \\mid Z)\n\\]\nNotation: \\(X \\perp Y \\mid Z\\)\nML relevance:\n\nNaive Bayes assumes features are conditionally independent given class\nGraphical models encode conditional independence"
  },
  {
    "objectID": "probability.html#bayes-theorem",
    "href": "probability.html#bayes-theorem",
    "title": "Probability",
    "section": "",
    "text": "Bayes’ Theorem lets us update what we believe about a situation after seeing new evidence.\nThe formula:\n\\[\nP(A \\mid B) = \\frac{P(B \\mid A) \\cdot P(A)}{P(B)}\n\\]\nComponents:\n\n\\(P(A \\mid B)\\): Posterior — probability of \\(A\\) given that \\(B\\) happened\n\\(P(B \\mid A)\\): Likelihood — how likely is \\(B\\) if \\(A\\) is true\n\\(P(A)\\): Prior — our initial belief about \\(A\\)\n\\(P(B)\\): Evidence — total probability of \\(B\\) happening under all possibilities\n\nExpanded form (Law of Total Probability):\n\\[\nP(A \\mid B) = \\frac{P(B \\mid A) P(A)}{P(B \\mid A)P(A) + P(B \\mid A^c)P(A^c)}\n\\]\n\n\nBayes’ Theorem is about updating your belief:\n\nStart with your prior belief \\(P(A)\\)\nThen observe new evidence \\((B)\\)\nUpdate your belief using how likely that evidence is under \\(A\\) (\\(P(B \\mid A)\\))\n\n\n\n\nSuppose a disease affects 1% of the population.\nYou take a test that is: - \\(P(\\text{Positive} \\mid \\text{Disease}) = 0.99\\) (true positive rate) - \\(P(\\text{Positive} \\mid \\text{No Disease}) = 0.05\\) (false positive rate)\nYou test positive. What is the chance you actually have the disease?\nWe want to compute:\n\\[\nP(\\text{Disease} \\mid \\text{Positive}) = \\frac{P(\\text{Positive} \\mid \\text{Disease}) \\cdot P(\\text{Disease})}{P(\\text{Positive})}\n\\]\nLet’s plug in values: - \\(P(\\text{Disease}) = 0.01\\) - \\(P(\\text{No Disease}) = 0.99\\) - \\(P(\\text{Positive}) = 0.99 \\cdot 0.01 + 0.05 \\cdot 0.99 = 0.0594\\)\nSo:\n\\[\nP(\\text{Disease} \\mid \\text{Positive}) = \\frac{0.99 \\cdot 0.01}{0.0594} \\approx 0.167\n\\]\nSurprising result: Even after a positive test, the chance of having the disease is only about 16.7%, because false positives are more common than true positives.\n\n\n\nBayes’ Theorem is widely used in:\n\nMedical diagnosis\nSpam detection\nProbabilistic machine learning (e.g., Naive Bayes classifiers)\nUpdating beliefs in AI models\nA/B testing and experimental design\n\n\n\n\nIn many machine learning applications, the denominator \\(P(B)\\) is the same for all outcomes and can be ignored:\n\\[\nP(A \\mid B) \\propto P(B \\mid A) \\cdot P(A)\n\\]\nThis version is often used for ranking outcomes instead of calculating exact probabilities.\nML relevance: Naive Bayes classification, Bayesian inference"
  },
  {
    "objectID": "probability.html#law-of-total-probability",
    "href": "probability.html#law-of-total-probability",
    "title": "Probability",
    "section": "",
    "text": "For a partition of the sample space \\(\\{A_1, A_2, \\ldots, A_n\\}\\):\n\\[\nP(B) = \\sum_{i=1}^n P(B \\mid A_i) P(A_i)\n\\]\nContinuous version:\n\\[\nP(B) = \\int P(B \\mid A = a) P(A = a) \\, da\n\\]\nML relevance: Computing marginal probabilities, evidence in Bayesian inference"
  },
  {
    "objectID": "probability.html#probability-distributions",
    "href": "probability.html#probability-distributions",
    "title": "Probability",
    "section": "",
    "text": "Probability distributions describe how values of a random variable are distributed.\n\n\n\n\nSingle binary trial (success/failure):\n\\[\nP(X = 1) = p, \\quad P(X = 0) = 1 - p\n\\]\n\n\\(\\mathbb{E}[X] = p\\)\n\\(\\text{Var}(X) = p(1-p)\\)\n\nML relevance: Binary classification outputs\n\n\n\nNumber of successes in \\(n\\) Bernoulli trials:\n\\[\nP(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}\n\\]\n\n\\(\\mathbb{E}[X] = np\\)\n\\(\\text{Var}(X) = np(1-p)\\)\n\n\n\n\nGeneralization of Bernoulli to \\(k\\) categories:\n\\[\nP(X = i) = p_i, \\quad \\sum_{i=1}^k p_i = 1\n\\]\nML relevance: Multi-class classification\n\n\n\nNumber of events in fixed interval:\n\\[\nP(X = k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\n\\]\n\n\\(\\mathbb{E}[X] = \\lambda\\)\n\\(\\text{Var}(X) = \\lambda\\)\n\nML relevance: Count data, rare events\n\n\n\n\n\n\nEqual probability over interval \\([a, b]\\):\n\\[\nf(x) = \\frac{1}{b - a}, \\quad a \\leq x \\leq b\n\\]\n\n\\(\\mathbb{E}[X] = \\frac{a + b}{2}\\)\n\\(\\text{Var}(X) = \\frac{(b-a)^2}{12}\\)\n\nML relevance: Random initialization, data augmentation\n\n\n\nTime between events in Poisson process:\n\\[\nf(x) = \\lambda e^{-\\lambda x}, \\quad x \\geq 0\n\\]\n\n\\(\\mathbb{E}[X] = \\frac{1}{\\lambda}\\)\n\\(\\text{Var}(X) = \\frac{1}{\\lambda^2}\\)\n\n\n\n\nPDF of normal distribution in 1D:\n\\[\nf(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\n\\]\nNotation: \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\)\nParameters: - \\(\\mu\\): mean - \\(\\sigma^2\\): variance - \\(\\sigma\\): standard deviation\nProperties:\n\nSymmetric around \\(\\mu\\)\n68-95-99.7 rule: ~68% within 1σ, ~95% within 2σ, ~99.7% within 3σ\nSum of independent Gaussians is Gaussian\nCentral Limit Theorem: sum of many i.i.d. variables → Gaussian\n\nStandard Normal: \\(\\mathcal{N}(0, 1)\\)\n\\[\nZ = \\frac{X - \\mu}{\\sigma} \\sim \\mathcal{N}(0, 1)\n\\]\n\n\n\nThe normal (Gaussian) distribution can be extended to multiple variables — for example, when \\(\\mathbf{x}\\) is a vector instead of just a number.\nWhen \\(\\mathbf{x}\\) is a vector in \\(\\mathbb{R}^d\\) (i.e., a list of \\(d\\) values), the multivariate Gaussian looks like:\n\\[\n\\mathcal{N}(\\mathbf{x} \\mid \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\frac{1}{(2\\pi)^{d/2}|\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left(-\\frac{1}{2}(\\mathbf{x} - \\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1}(\\mathbf{x} - \\boldsymbol{\\mu})\\right)\n\\]\nWhat do all these symbols mean?\n\n\\(\\mathbf{x}\\): A \\(d\\)-dimensional vector (e.g., an image flattened into a 1D array)\n\\(\\boldsymbol{\\mu}\\): The mean vector (the “center” of the distribution)\n\\(\\boldsymbol{\\Sigma}\\): The \\(d \\times d\\) covariance matrix, which captures the spread and correlation of the variables\n\\(|\\boldsymbol{\\Sigma}|\\): The determinant of \\(\\boldsymbol{\\Sigma}\\), representing the volume scaling\n\\((2\\pi)^{d/2}\\): Comes from extending the 1D normalizing constant to \\(d\\) dimensions\n\\((\\mathbf{x} - \\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1}(\\mathbf{x} - \\boldsymbol{\\mu})\\): A generalized squared distance from \\(\\mathbf{x}\\) to the mean — see below!\n\nWhat’s the Mahalanobis Distance?\nThe term \\((\\mathbf{x} - \\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1}(\\mathbf{x} - \\boldsymbol{\\mu})\\) measures how far \\(\\mathbf{x}\\) is from the mean \\(\\boldsymbol{\\mu}\\), taking into account how spread out the data is in different directions. It’s like Euclidean distance, but adjusted for direction-dependent variance. The more variance in a direction, the less distance is penalized in that direction.\nSpecial Case: Spherical Gaussian\nIf all variables are independent and have equal variance \\(\\sigma^2\\), then \\(\\boldsymbol{\\Sigma} = \\sigma^2 \\mathbf{I}\\) and the formula simplifies to:\n\\[\n\\mathcal{N}(\\mathbf{x} \\mid \\boldsymbol{\\mu}, \\sigma^2 \\mathbf{I}) = \\frac{1}{(2\\pi\\sigma^2)^{d/2}} \\exp\\left(-\\frac{1}{2\\sigma^2}\\|\\mathbf{x} - \\boldsymbol{\\mu}\\|^2\\right)\n\\]\nThis looks more like the familiar 1D bell curve — but extended to \\(d\\) dimensions.\nML relevance:\n\nGaussian Mixture Models (GMM)\nLatent variable models (VAE)\nGaussian processes\nDiscriminant analysis\n\n\n\n\nProbability distribution over probabilities \\([0, 1]\\):\n\\[\nf(x) = \\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{B(\\alpha, \\beta)}, \\quad 0 \\leq x \\leq 1\n\\]\nwhere \\(B(\\alpha, \\beta)\\) is the beta function\nML relevance: Bayesian inference (conjugate prior for Bernoulli/Binomial)\n\n\n\nGeneralization of exponential distribution:\n\\[\nf(x) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} x^{\\alpha-1} e^{-\\beta x}, \\quad x \\geq 0\n\\]\nML relevance: Prior distributions in Bayesian models"
  },
  {
    "objectID": "probability.html#maximum-likelihood-estimation-mle",
    "href": "probability.html#maximum-likelihood-estimation-mle",
    "title": "Probability",
    "section": "",
    "text": "Used to estimate parameters of models:\n\\[\n\\hat{\\theta}_{\\text{MLE}} = \\arg\\max_\\theta P(\\mathcal{D} \\mid \\theta)\n\\]\nEquivalently (using log-likelihood):\n\\[\n\\hat{\\theta}_{\\text{MLE}} = \\arg\\max_\\theta \\sum_{i=1}^n \\log P(x_i \\mid \\theta)\n\\]\nWhy log-likelihood?\n\nProducts become sums (easier to optimize)\nNumerically more stable\nSame maximum as likelihood\n\nExample: MLE for Gaussian\nGiven data \\(\\{x_1, \\ldots, x_n\\}\\) from \\(\\mathcal{N}(\\mu, \\sigma^2)\\):\n\\[\n\\hat{\\mu}_{\\text{MLE}} = \\frac{1}{n}\\sum_{i=1}^n x_i \\quad \\text{(sample mean)}\n\\]\n\\[\n\\hat{\\sigma}^2_{\\text{MLE}} = \\frac{1}{n}\\sum_{i=1}^n (x_i - \\hat{\\mu})^2 \\quad \\text{(sample variance)}\n\\]\nML relevance: Training probabilistic models, logistic regression"
  },
  {
    "objectID": "probability.html#maximum-a-posteriori-map-estimation",
    "href": "probability.html#maximum-a-posteriori-map-estimation",
    "title": "Probability",
    "section": "",
    "text": "Incorporates prior knowledge:\n\\[\n\\hat{\\theta}_{\\text{MAP}} = \\arg\\max_\\theta P(\\theta \\mid \\mathcal{D}) = \\arg\\max_\\theta P(\\mathcal{D} \\mid \\theta) P(\\theta)\n\\]\nUsing log:\n\\[\n\\hat{\\theta}_{\\text{MAP}} = \\arg\\max_\\theta \\left[\\log P(\\mathcal{D} \\mid \\theta) + \\log P(\\theta)\\right]\n\\]\nDifference from MLE: MAP includes prior \\(P(\\theta)\\)\nML relevance:\n\nRegularization (L2 = Gaussian prior, L1 = Laplace prior)\nBayesian neural networks"
  },
  {
    "objectID": "probability.html#kl-divergence",
    "href": "probability.html#kl-divergence",
    "title": "Probability",
    "section": "",
    "text": "Measures how one distribution diverges from another:\n\\[\nD_{\\text{KL}}(P \\| Q) = \\sum_x P(x) \\log \\frac{P(x)}{Q(x)}\n\\]\nContinuous:\n\\[\nD_{\\text{KL}}(P \\| Q) = \\int P(x) \\log \\frac{P(x)}{Q(x)} \\, dx\n\\]\nProperties:\n\n\\(D_{\\text{KL}}(P \\| Q) \\geq 0\\) (Gibb’s inequality)\n\\(D_{\\text{KL}}(P \\| Q) = 0\\) iff \\(P = Q\\)\nNot symmetric: \\(D_{\\text{KL}}(P \\| Q) \\neq D_{\\text{KL}}(Q \\| P)\\)\nNot a true distance metric\n\nEquivalent form:\n\\[\nD_{\\text{KL}}(P \\| Q) = \\mathbb{E}_{x \\sim P}\\left[\\log \\frac{P(x)}{Q(x)}\\right] = \\mathbb{E}_{x \\sim P}[\\log P(x)] - \\mathbb{E}_{x \\sim P}[\\log Q(x)]\n\\]\n\\[\n= -H(P) - \\mathbb{E}_{x \\sim P}[\\log Q(x)]\n\\]\nwhere \\(H(P)\\) is the entropy of \\(P\\)\nML relevance:\n\nVariational inference\nTraining VAEs (ELBO)\nInformation-theoretic learning\nModel comparison"
  },
  {
    "objectID": "probability.html#cross-entropy",
    "href": "probability.html#cross-entropy",
    "title": "Probability",
    "section": "",
    "text": "Measures expected log-likelihood under distribution \\(Q\\) when true distribution is \\(P\\):\n\\[\nH(P, Q) = -\\sum_x P(x) \\log Q(x)\n\\]\nContinuous:\n\\[\nH(P, Q) = -\\int P(x) \\log Q(x) \\, dx\n\\]\nRelation to KL divergence:\n\\[\nH(P, Q) = H(P) + D_{\\text{KL}}(P \\| Q)\n\\]\nwhere \\(H(P) = -\\sum_x P(x) \\log P(x)\\) is the entropy\nML relevance:\n\nCross-entropy loss in classification\nMinimizing cross-entropy ≡ minimizing KL divergence (since \\(H(P)\\) is constant)"
  },
  {
    "objectID": "probability.html#entropy",
    "href": "probability.html#entropy",
    "title": "Probability",
    "section": "",
    "text": "Measures uncertainty or information content:\nShannon Entropy:\n\\[\nH(X) = -\\sum_x P(x) \\log P(x) = \\mathbb{E}[-\\log P(X)]\n\\]\nContinuous (Differential Entropy):\n\\[\nH(X) = -\\int f(x) \\log f(x) \\, dx\n\\]\nProperties:\n\n\\(H(X) \\geq 0\\)\nMaximum entropy for discrete uniform distribution\nHigher entropy = more uncertainty\n\nConditional Entropy:\n\\[\nH(Y \\mid X) = \\sum_x P(x) H(Y \\mid X = x)\n\\]\nML relevance:\n\nInformation gain in decision trees\nEntropy regularization\nInformation theory foundations"
  },
  {
    "objectID": "probability.html#mutual-information",
    "href": "probability.html#mutual-information",
    "title": "Probability",
    "section": "",
    "text": "Measures dependence between variables:\n\\[\nI(X; Y) = D_{\\text{KL}}(P(X,Y) \\| P(X)P(Y))\n\\]\nEquivalent forms:\n\\[\nI(X; Y) = H(X) - H(X \\mid Y) = H(Y) - H(Y \\mid X)\n\\]\n\\[\nI(X; Y) = H(X) + H(Y) - H(X, Y)\n\\]\nProperties:\n\n\\(I(X; Y) \\geq 0\\)\n\\(I(X; Y) = 0\\) iff \\(X\\) and \\(Y\\) independent\nSymmetric: \\(I(X; Y) = I(Y; X)\\)\n\nML relevance:\n\nFeature selection\nInformation bottleneck theory\nVariational information maximization"
  },
  {
    "objectID": "probability.html#central-limit-theorem-clt",
    "href": "probability.html#central-limit-theorem-clt",
    "title": "Probability",
    "section": "",
    "text": "For i.i.d. random variables \\(X_1, \\ldots, X_n\\) with mean \\(\\mu\\) and variance \\(\\sigma^2\\):\n\\[\n\\frac{\\bar{X}_n - \\mu}{\\sigma/\\sqrt{n}} \\xrightarrow{d} \\mathcal{N}(0, 1)\n\\]\nwhere \\(\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^n X_i\\)\nPractical form: For large \\(n\\):\n\\[\n\\bar{X}_n \\approx \\mathcal{N}\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\n\\]\nML relevance:\n\nJustifies Gaussian assumptions\nConfidence intervals\nBootstrap methods"
  },
  {
    "objectID": "probability.html#law-of-large-numbers-lln",
    "href": "probability.html#law-of-large-numbers-lln",
    "title": "Probability",
    "section": "",
    "text": "For i.i.d. \\(X_1, \\ldots, X_n\\) with mean \\(\\mu\\):\n\\[\n\\bar{X}_n \\xrightarrow{P} \\mu\n\\]\n(Sample mean converges in probability to true mean)\n\n\n\n\\[\n\\bar{X}_n \\xrightarrow{\\text{a.s.}} \\mu\n\\]\n(Sample mean converges almost surely)\nML relevance:\n\nMonte Carlo methods\nJustifies empirical risk minimization"
  },
  {
    "objectID": "probability.html#jensens-inequality",
    "href": "probability.html#jensens-inequality",
    "title": "Probability",
    "section": "",
    "text": "For convex function \\(f\\) and random variable \\(X\\):\n\\[\nf(\\mathbb{E}[X]) \\leq \\mathbb{E}[f(X)]\n\\]\nFor concave function: reverse inequality\nExample: \\(\\log\\) is concave, so:\n\\[\n\\log(\\mathbb{E}[X]) \\geq \\mathbb{E}[\\log(X)]\n\\]\nML relevance:\n\nDerives EM algorithm\nVariational inference bounds\nInformation theory inequalities"
  },
  {
    "objectID": "probability.html#classification",
    "href": "probability.html#classification",
    "title": "Probability",
    "section": "2.1 Classification",
    "text": "2.1 Classification\nLogistic Regression (binary):\n\\[\nP(y = 1 \\mid \\mathbf{x}) = \\sigma(\\mathbf{w}^T\\mathbf{x}) = \\frac{1}{1 + e^{-\\mathbf{w}^T\\mathbf{x}}}\n\\]\nSoftmax (multi-class):\n\\[\nP(y = k \\mid \\mathbf{x}) = \\frac{e^{\\mathbf{w}_k^T\\mathbf{x}}}{\\sum_{j=1}^K e^{\\mathbf{w}_j^T\\mathbf{x}}}\n\\]"
  },
  {
    "objectID": "probability.html#naive-bayes-classifier",
    "href": "probability.html#naive-bayes-classifier",
    "title": "Probability",
    "section": "2.2 Naive Bayes Classifier",
    "text": "2.2 Naive Bayes Classifier\nAssumes features conditionally independent:\n\\[\nP(y \\mid \\mathbf{x}) \\propto P(y) \\prod_{i=1}^d P(x_i \\mid y)\n\\]\nDecision rule:\n\\[\n\\hat{y} = \\arg\\max_y P(y) \\prod_{i=1}^d P(x_i \\mid y)\n\\]"
  },
  {
    "objectID": "probability.html#gaussian-mixture-models-gmm",
    "href": "probability.html#gaussian-mixture-models-gmm",
    "title": "Probability",
    "section": "2.3 Gaussian Mixture Models (GMM)",
    "text": "2.3 Gaussian Mixture Models (GMM)\nMixture of \\(K\\) Gaussians:\n\\[\nP(\\mathbf{x}) = \\sum_{k=1}^K \\pi_k \\mathcal{N}(\\mathbf{x} \\mid \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)\n\\]\nwhere \\(\\sum_{k=1}^K \\pi_k = 1\\) (mixing coefficients)\nML relevance: Clustering, density estimation, generative models"
  },
  {
    "objectID": "probability.html#expectation-maximization-em-algorithm",
    "href": "probability.html#expectation-maximization-em-algorithm",
    "title": "Probability",
    "section": "2.4 Expectation-Maximization (EM) Algorithm",
    "text": "2.4 Expectation-Maximization (EM) Algorithm\nIterative method for MLE with latent variables:\nE-step: Compute expected log-likelihood\n\\[\nQ(\\theta \\mid \\theta^{(t)}) = \\mathbb{E}_{Z \\mid X, \\theta^{(t)}}[\\log P(X, Z \\mid \\theta)]\n\\]\nM-step: Maximize w.r.t. \\(\\theta\\)\n\\[\n\\theta^{(t+1)} = \\arg\\max_\\theta Q(\\theta \\mid \\theta^{(t)})\n\\]\nML relevance: Training GMMs, hidden Markov models"
  },
  {
    "objectID": "probability.html#sampling-methods",
    "href": "probability.html#sampling-methods",
    "title": "Probability",
    "section": "2.5 Sampling Methods",
    "text": "2.5 Sampling Methods\n\n2.5.1 Monte Carlo Estimation\nApproximate expectation by sampling:\n\\[\n\\mathbb{E}[f(X)] \\approx \\frac{1}{N}\\sum_{i=1}^N f(x_i), \\quad x_i \\sim P(X)\n\\]\n\n\n2.5.2 Markov Chain Monte Carlo (MCMC)\nGenerate samples from complex distributions:\n\nMetropolis-Hastings: Accept/reject samples based on ratio\nGibbs Sampling: Sample each variable conditional on others\n\nML relevance: Bayesian inference, probabilistic programming"
  },
  {
    "objectID": "probability.html#quick-reference-table",
    "href": "probability.html#quick-reference-table",
    "title": "Probability",
    "section": "2.6 Quick Reference Table",
    "text": "2.6 Quick Reference Table\n\n\n\n\n\n\n\n\nConcept\nFormula\nML Application\n\n\n\n\nBayes’ Theorem\n\\(P(A \\mid B) = \\frac{P(B \\mid A)P(A)}{P(B)}\\)\nClassification, inference\n\n\nKL Divergence\n\\(D_{\\text{KL}}(P \\| Q) = \\sum P(x)\\log\\frac{P(x)}{Q(x)}\\)\nVAE training, model comparison\n\n\nCross-Entropy\n\\(H(P,Q) = -\\sum P(x)\\log Q(x)\\)\nClassification loss\n\n\nEntropy\n\\(H(X) = -\\sum P(x)\\log P(x)\\)\nInformation theory, decision trees\n\n\nGaussian PDF\n\\(\\mathcal{N}(\\mu, \\sigma^2)\\)\nContinuous modeling\n\n\nMLE\n\\(\\hat{\\theta} = \\arg\\max P(\\mathcal{D} \\mid \\theta)\\)\nParameter estimation\n\n\nMAP\n\\(\\hat{\\theta} = \\arg\\max P(\\theta \\mid \\mathcal{D})\\)\nBayesian estimation\n\n\nCovariance\n\\(\\text{Cov}(X,Y) = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y]\\)\nFeature correlation\n\n\nCLT\n\\(\\bar{X}_n \\to \\mathcal{N}(\\mu, \\sigma^2/n)\\)\nStatistical inference"
  },
  {
    "objectID": "calculus.html",
    "href": "calculus.html",
    "title": "Calculus",
    "section": "",
    "text": "This section covers core calculus concepts used in machine learning — especially in optimization, backpropagation, and probability.\n\n\nA general method for rewriting quadratic expressions:\n\\[\nx^2 - 2bx + c = \\left(x - b\\right)^2 + \\frac{b^2}{c} - \\frac{b^2}{a}\n\\]\nThis transformation is widely used in:\n\nGaussian probability density derivations\nKL divergence simplification\nOptimization of quadratic objectives\n\n\n\n\nThe derivative of a function measures the rate of change:\n\\[\n\\frac{d}{dx} f(x)\n\\]\nCommon derivatives:\n\n\\(\\frac{d}{dx}(x^n) = nx^{n-1}\\) (power rule)\n\\(\\frac{d}{dx}(e^x) = e^x\\)\n\\(\\frac{d}{dx}(\\ln x) = \\frac{1}{x}\\)\n\\(\\frac{d}{dx}(\\sin x) = \\cos x\\)\n\\(\\frac{d}{dx}(\\cos x) = -\\sin x\\)\n\\(\\frac{d}{dx}(\\tanh x) = 1 - \\tanh^2 x\\) (used in activation functions)\n\nRules:\n\nProduct rule: \\((fg)' = f'g + fg'\\)\nQuotient rule: \\(\\left(\\frac{f}{g}\\right)' = \\frac{f'g - fg'}{g^2}\\)\nChain rule: \\(\\frac{d}{dx}f(g(x)) = f'(g(x)) \\cdot g'(x)\\)\n\n\n\n\nUsed when dealing with multivariable functions:\n\\[\n\\frac{\\partial f}{\\partial x}, \\quad \\frac{\\partial f}{\\partial y}\n\\]\nExample: For \\(f(x,y) = x^2y + 3y^2\\): - \\(\\frac{\\partial f}{\\partial x} = 2xy\\) - \\(\\frac{\\partial f}{\\partial y} = x^2 + 6y\\)\nML relevance: Computing gradients for each parameter in neural networks\n\n\n\nThe gradient is a vector of all partial derivatives:\n\\[\n\\nabla f = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_n} \\end{bmatrix}\n\\]\nProperties:\n\nPoints in direction of steepest ascent\nMagnitude indicates steepness\nPerpendicular to level curves/surfaces\n\nML relevance: Gradient descent uses \\(-\\nabla f\\) to minimize loss functions\n\n\n\nFor a vector-valued function \\(\\mathbf{f}: \\mathbb{R}^n \\to \\mathbb{R}^m\\), the Jacobian is:\n\\[\nJ_{ij} = \\frac{\\partial f_i}{\\partial x_j}\n\\]\nDimensions: \\(m \\times n\\) matrix\nExample: For \\(\\mathbf{f}(x,y) = [x^2y, xy^2]\\):\n\\[\nJ = \\begin{bmatrix}\n2xy & x^2 \\\\\ny^2 & 2xy\n\\end{bmatrix}\n\\]\nML relevance:\n\nBackpropagation through layers\nComputing derivatives of vector outputs\n\n\n\n\nSecond-order partial derivatives:\n\\[\nH_{ij} = \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}\n\\]\nProperties:\n\nSymmetric matrix (if \\(f\\) is twice differentiable)\nDiagonal elements: \\(\\frac{\\partial^2 f}{\\partial x_i^2}\\)\nOff-diagonal: mixed partials \\(\\frac{\\partial^2 f}{\\partial x_i \\partial x_j}\\)\n\nUsed in curvature analysis and second-order optimization.\nML relevance:\n\nNewton’s method optimization\nAnalyzing convergence properties\nIdentifying saddle points vs local minima\n\n\n\n\nFor composed functions \\(f(g(x))\\):\n\\[\n\\frac{\\partial f}{\\partial x_i} = \\sum_j \\frac{\\partial f}{\\partial g_j} \\cdot \\frac{\\partial g_j}{\\partial x_i}\n\\]\nMatrix form:\n\\[\n\\frac{\\partial f}{\\partial \\mathbf{x}} = \\frac{\\partial f}{\\partial \\mathbf{g}} \\cdot \\frac{\\partial \\mathbf{g}}{\\partial \\mathbf{x}}\n\\]\nML relevance: Foundation of backpropagation algorithm\n\n\n\nRate of change of \\(f\\) in direction of unit vector \\(\\mathbf{u}\\):\n\\[\nD_\\mathbf{u} f = \\nabla f \\cdot \\mathbf{u}\n\\]\nProperties:\n\nMaximum when \\(\\mathbf{u}\\) aligned with \\(\\nabla f\\)\nZero when \\(\\mathbf{u}\\) perpendicular to \\(\\nabla f\\)\n\nML relevance: Understanding optimization landscapes\n\n\n\nApproximating functions using derivatives:\n\\[\nf(x + \\Delta x) \\approx f(x) + f'(x)\\Delta x + \\frac{1}{2}f''(x)\\Delta x^2 + \\cdots\n\\]\nFirst-order (linear) approximation:\n\\[\nf(x + \\Delta x) \\approx f(x) + f'(x)\\Delta x\n\\]\nSecond-order (quadratic) approximation:\n\\[\nf(x + \\Delta x) \\approx f(x) + f'(x)\\Delta x + \\frac{1}{2}f''(x)\\Delta x^2\n\\]\nMultivariate Taylor expansion:\n\\[\nf(\\mathbf{x} + \\mathbf{\\Delta x}) \\approx f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^T \\mathbf{\\Delta x} + \\frac{1}{2}\\mathbf{\\Delta x}^T H \\mathbf{\\Delta x}\n\\]\nML relevance:\n\nLocal approximations in optimization\nNewton’s method derivation\nTrust region methods\n\n\n\n\nArea under a curve:\n\\[\n\\int_a^b f(x) \\, dx\n\\]\nCommon integrals:\n\n\\(\\int x^n \\, dx = \\frac{x^{n+1}}{n+1} + C\\) (for \\(n \\neq -1\\))\n\\(\\int e^x \\, dx = e^x + C\\)\n\\(\\int \\frac{1}{x} \\, dx = \\ln|x| + C\\)\n\\(\\int \\sin x \\, dx = -\\cos x + C\\)\n\\(\\int \\cos x \\, dx = \\sin x + C\\)\n\nML relevance:\n\nComputing expectations in probability\nNormalizing probability distributions\nDeriving closed-form solutions\n\n\n\n\nA useful transformation:\n\\[\n\\int u \\, dv = uv - \\int v \\, du\n\\]\nML relevance:\n\nDeriving variational bounds\nEvidence lower bound (ELBO) derivations\n\n\n\n\nLinks differentiation and integration:\n\\[\n\\frac{d}{dx} \\int_a^x f(t) \\, dt = f(x)\n\\]\n\\[\n\\int_a^b f'(x) \\, dx = f(b) - f(a)\n\\]\n\n\n\n\n\nAt a local minimum/maximum \\(x^*\\):\n\\[\n\\nabla f(x^*) = \\mathbf{0}\n\\]\nCritical points: Where gradient vanishes\n\n\n\nFor a local minimum at \\(x^*\\):\n\n\\(\\nabla f(x^*) = \\mathbf{0}\\) (first-order condition)\nHessian \\(H(x^*)\\) is positive definite (all eigenvalues &gt; 0)\n\nFor a local maximum: - Hessian \\(H(x^*)\\) is negative definite (all eigenvalues &lt; 0)\nSaddle point: Hessian has both positive and negative eigenvalues\nML relevance: Analyzing loss function landscapes\n\n\n\n\nA function \\(f\\) is convex if:\n\\[\nf(\\lambda x + (1-\\lambda)y) \\leq \\lambda f(x) + (1-\\lambda)f(y)\n\\]\nfor all \\(x, y\\) and \\(\\lambda \\in [0,1]\\)\nEquivalent conditions:\n\n\\(f''(x) \\geq 0\\) (for univariate functions)\nHessian \\(H\\) is positive semi-definite (for multivariate)\nAny local minimum is a global minimum\n\nStrictly convex: Use strict inequalities (\\(&gt;\\) instead of \\(\\geq\\))\nML relevance:\n\nGuarantees for optimization convergence\nLinear regression, logistic regression are convex\nNeural networks are generally non-convex\n\n\n\n\nFor constrained optimization:\n\\[\n\\min f(\\mathbf{x}) \\quad \\text{subject to} \\quad g(\\mathbf{x}) = 0\n\\]\nMethod: Solve:\n\\[\n\\nabla f(\\mathbf{x}) = \\lambda \\nabla g(\\mathbf{x})\n\\]\nand \\(g(\\mathbf{x}) = 0\\)\nLagrangian:\n\\[\n\\mathcal{L}(\\mathbf{x}, \\lambda) = f(\\mathbf{x}) - \\lambda g(\\mathbf{x})\n\\]\nML relevance:\n\nSupport Vector Machines (SVM)\nConstrained optimization problems\nDual formulations\n\n\n\n\nFor indeterminate forms \\(\\frac{0}{0}\\) or \\(\\frac{\\infty}{\\infty}\\):\n\\[\n\\lim_{x \\to c} \\frac{f(x)}{g(x)} = \\lim_{x \\to c} \\frac{f'(x)}{g'(x)}\n\\]\nML relevance: Analyzing limiting behavior of loss functions\n\n\n\nSigmoid: \\(\\sigma(x) = \\frac{1}{1 + e^{-x}}\\)\n\\[\n\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))\n\\]\nTanh: \\(\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\\)\n\\[\n\\tanh'(x) = 1 - \\tanh^2(x)\n\\]\nReLU: \\(\\text{ReLU}(x) = \\max(0, x)\\)\n\\[\n\\text{ReLU}'(x) = \\begin{cases} 1 & \\text{if } x &gt; 0 \\\\ 0 & \\text{if } x &lt; 0 \\\\ \\text{undefined} & \\text{if } x = 0 \\end{cases}\n\\]\nSoftmax: For vector \\(\\mathbf{z}\\), \\(\\text{softmax}(\\mathbf{z})_i = \\frac{e^{z_i}}{\\sum_j e^{z_j}}\\)\n\\[\n\\frac{\\partial \\text{softmax}(\\mathbf{z})_i}{\\partial z_j} = \\text{softmax}(\\mathbf{z})_i (\\delta_{ij} - \\text{softmax}(\\mathbf{z})_j)\n\\]\nwhere \\(\\delta_{ij}\\) is the Kronecker delta"
  },
  {
    "objectID": "calculus.html#completing-the-square",
    "href": "calculus.html#completing-the-square",
    "title": "Calculus",
    "section": "",
    "text": "A general method for rewriting quadratic expressions:\n\\[\nx^2 - 2bx + c = \\left(x - b\\right)^2 + \\frac{b^2}{c} - \\frac{b^2}{a}\n\\]\nThis transformation is widely used in:\n\nGaussian probability density derivations\nKL divergence simplification\nOptimization of quadratic objectives"
  },
  {
    "objectID": "calculus.html#derivatives",
    "href": "calculus.html#derivatives",
    "title": "Calculus",
    "section": "",
    "text": "The derivative of a function measures the rate of change:\n\\[\n\\frac{d}{dx} f(x)\n\\]\nCommon derivatives:\n\n\\(\\frac{d}{dx}(x^n) = nx^{n-1}\\) (power rule)\n\\(\\frac{d}{dx}(e^x) = e^x\\)\n\\(\\frac{d}{dx}(\\ln x) = \\frac{1}{x}\\)\n\\(\\frac{d}{dx}(\\sin x) = \\cos x\\)\n\\(\\frac{d}{dx}(\\cos x) = -\\sin x\\)\n\\(\\frac{d}{dx}(\\tanh x) = 1 - \\tanh^2 x\\) (used in activation functions)\n\nRules:\n\nProduct rule: \\((fg)' = f'g + fg'\\)\nQuotient rule: \\(\\left(\\frac{f}{g}\\right)' = \\frac{f'g - fg'}{g^2}\\)\nChain rule: \\(\\frac{d}{dx}f(g(x)) = f'(g(x)) \\cdot g'(x)\\)"
  },
  {
    "objectID": "calculus.html#partial-derivatives",
    "href": "calculus.html#partial-derivatives",
    "title": "Calculus",
    "section": "",
    "text": "Used when dealing with multivariable functions:\n\\[\n\\frac{\\partial f}{\\partial x}, \\quad \\frac{\\partial f}{\\partial y}\n\\]\nExample: For \\(f(x,y) = x^2y + 3y^2\\): - \\(\\frac{\\partial f}{\\partial x} = 2xy\\) - \\(\\frac{\\partial f}{\\partial y} = x^2 + 6y\\)\nML relevance: Computing gradients for each parameter in neural networks"
  },
  {
    "objectID": "calculus.html#gradient-vector",
    "href": "calculus.html#gradient-vector",
    "title": "Calculus",
    "section": "",
    "text": "The gradient is a vector of all partial derivatives:\n\\[\n\\nabla f = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_n} \\end{bmatrix}\n\\]\nProperties:\n\nPoints in direction of steepest ascent\nMagnitude indicates steepness\nPerpendicular to level curves/surfaces\n\nML relevance: Gradient descent uses \\(-\\nabla f\\) to minimize loss functions"
  },
  {
    "objectID": "calculus.html#jacobian-matrix",
    "href": "calculus.html#jacobian-matrix",
    "title": "Calculus",
    "section": "",
    "text": "For a vector-valued function \\(\\mathbf{f}: \\mathbb{R}^n \\to \\mathbb{R}^m\\), the Jacobian is:\n\\[\nJ_{ij} = \\frac{\\partial f_i}{\\partial x_j}\n\\]\nDimensions: \\(m \\times n\\) matrix\nExample: For \\(\\mathbf{f}(x,y) = [x^2y, xy^2]\\):\n\\[\nJ = \\begin{bmatrix}\n2xy & x^2 \\\\\ny^2 & 2xy\n\\end{bmatrix}\n\\]\nML relevance:\n\nBackpropagation through layers\nComputing derivatives of vector outputs"
  },
  {
    "objectID": "calculus.html#hessian-matrix",
    "href": "calculus.html#hessian-matrix",
    "title": "Calculus",
    "section": "",
    "text": "Second-order partial derivatives:\n\\[\nH_{ij} = \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}\n\\]\nProperties:\n\nSymmetric matrix (if \\(f\\) is twice differentiable)\nDiagonal elements: \\(\\frac{\\partial^2 f}{\\partial x_i^2}\\)\nOff-diagonal: mixed partials \\(\\frac{\\partial^2 f}{\\partial x_i \\partial x_j}\\)\n\nUsed in curvature analysis and second-order optimization.\nML relevance:\n\nNewton’s method optimization\nAnalyzing convergence properties\nIdentifying saddle points vs local minima"
  },
  {
    "objectID": "calculus.html#chain-rule-multivariate",
    "href": "calculus.html#chain-rule-multivariate",
    "title": "Calculus",
    "section": "",
    "text": "For composed functions \\(f(g(x))\\):\n\\[\n\\frac{\\partial f}{\\partial x_i} = \\sum_j \\frac{\\partial f}{\\partial g_j} \\cdot \\frac{\\partial g_j}{\\partial x_i}\n\\]\nMatrix form:\n\\[\n\\frac{\\partial f}{\\partial \\mathbf{x}} = \\frac{\\partial f}{\\partial \\mathbf{g}} \\cdot \\frac{\\partial \\mathbf{g}}{\\partial \\mathbf{x}}\n\\]\nML relevance: Foundation of backpropagation algorithm"
  },
  {
    "objectID": "calculus.html#directional-derivative",
    "href": "calculus.html#directional-derivative",
    "title": "Calculus",
    "section": "",
    "text": "Rate of change of \\(f\\) in direction of unit vector \\(\\mathbf{u}\\):\n\\[\nD_\\mathbf{u} f = \\nabla f \\cdot \\mathbf{u}\n\\]\nProperties:\n\nMaximum when \\(\\mathbf{u}\\) aligned with \\(\\nabla f\\)\nZero when \\(\\mathbf{u}\\) perpendicular to \\(\\nabla f\\)\n\nML relevance: Understanding optimization landscapes"
  },
  {
    "objectID": "calculus.html#taylor-series-expansion",
    "href": "calculus.html#taylor-series-expansion",
    "title": "Calculus",
    "section": "",
    "text": "Approximating functions using derivatives:\n\\[\nf(x + \\Delta x) \\approx f(x) + f'(x)\\Delta x + \\frac{1}{2}f''(x)\\Delta x^2 + \\cdots\n\\]\nFirst-order (linear) approximation:\n\\[\nf(x + \\Delta x) \\approx f(x) + f'(x)\\Delta x\n\\]\nSecond-order (quadratic) approximation:\n\\[\nf(x + \\Delta x) \\approx f(x) + f'(x)\\Delta x + \\frac{1}{2}f''(x)\\Delta x^2\n\\]\nMultivariate Taylor expansion:\n\\[\nf(\\mathbf{x} + \\mathbf{\\Delta x}) \\approx f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^T \\mathbf{\\Delta x} + \\frac{1}{2}\\mathbf{\\Delta x}^T H \\mathbf{\\Delta x}\n\\]\nML relevance:\n\nLocal approximations in optimization\nNewton’s method derivation\nTrust region methods"
  },
  {
    "objectID": "calculus.html#integration",
    "href": "calculus.html#integration",
    "title": "Calculus",
    "section": "",
    "text": "Area under a curve:\n\\[\n\\int_a^b f(x) \\, dx\n\\]\nCommon integrals:\n\n\\(\\int x^n \\, dx = \\frac{x^{n+1}}{n+1} + C\\) (for \\(n \\neq -1\\))\n\\(\\int e^x \\, dx = e^x + C\\)\n\\(\\int \\frac{1}{x} \\, dx = \\ln|x| + C\\)\n\\(\\int \\sin x \\, dx = -\\cos x + C\\)\n\\(\\int \\cos x \\, dx = \\sin x + C\\)\n\nML relevance:\n\nComputing expectations in probability\nNormalizing probability distributions\nDeriving closed-form solutions"
  },
  {
    "objectID": "calculus.html#integration-by-parts",
    "href": "calculus.html#integration-by-parts",
    "title": "Calculus",
    "section": "",
    "text": "A useful transformation:\n\\[\n\\int u \\, dv = uv - \\int v \\, du\n\\]\nML relevance:\n\nDeriving variational bounds\nEvidence lower bound (ELBO) derivations"
  },
  {
    "objectID": "calculus.html#fundamental-theorem-of-calculus",
    "href": "calculus.html#fundamental-theorem-of-calculus",
    "title": "Calculus",
    "section": "",
    "text": "Links differentiation and integration:\n\\[\n\\frac{d}{dx} \\int_a^x f(t) \\, dt = f(x)\n\\]\n\\[\n\\int_a^b f'(x) \\, dx = f(b) - f(a)\n\\]"
  },
  {
    "objectID": "calculus.html#optimization-conditions",
    "href": "calculus.html#optimization-conditions",
    "title": "Calculus",
    "section": "",
    "text": "At a local minimum/maximum \\(x^*\\):\n\\[\n\\nabla f(x^*) = \\mathbf{0}\n\\]\nCritical points: Where gradient vanishes\n\n\n\nFor a local minimum at \\(x^*\\):\n\n\\(\\nabla f(x^*) = \\mathbf{0}\\) (first-order condition)\nHessian \\(H(x^*)\\) is positive definite (all eigenvalues &gt; 0)\n\nFor a local maximum: - Hessian \\(H(x^*)\\) is negative definite (all eigenvalues &lt; 0)\nSaddle point: Hessian has both positive and negative eigenvalues\nML relevance: Analyzing loss function landscapes"
  },
  {
    "objectID": "calculus.html#convexity",
    "href": "calculus.html#convexity",
    "title": "Calculus",
    "section": "",
    "text": "A function \\(f\\) is convex if:\n\\[\nf(\\lambda x + (1-\\lambda)y) \\leq \\lambda f(x) + (1-\\lambda)f(y)\n\\]\nfor all \\(x, y\\) and \\(\\lambda \\in [0,1]\\)\nEquivalent conditions:\n\n\\(f''(x) \\geq 0\\) (for univariate functions)\nHessian \\(H\\) is positive semi-definite (for multivariate)\nAny local minimum is a global minimum\n\nStrictly convex: Use strict inequalities (\\(&gt;\\) instead of \\(\\geq\\))\nML relevance:\n\nGuarantees for optimization convergence\nLinear regression, logistic regression are convex\nNeural networks are generally non-convex"
  },
  {
    "objectID": "calculus.html#lagrange-multipliers",
    "href": "calculus.html#lagrange-multipliers",
    "title": "Calculus",
    "section": "",
    "text": "For constrained optimization:\n\\[\n\\min f(\\mathbf{x}) \\quad \\text{subject to} \\quad g(\\mathbf{x}) = 0\n\\]\nMethod: Solve:\n\\[\n\\nabla f(\\mathbf{x}) = \\lambda \\nabla g(\\mathbf{x})\n\\]\nand \\(g(\\mathbf{x}) = 0\\)\nLagrangian:\n\\[\n\\mathcal{L}(\\mathbf{x}, \\lambda) = f(\\mathbf{x}) - \\lambda g(\\mathbf{x})\n\\]\nML relevance:\n\nSupport Vector Machines (SVM)\nConstrained optimization problems\nDual formulations"
  },
  {
    "objectID": "calculus.html#lhôpitals-rule",
    "href": "calculus.html#lhôpitals-rule",
    "title": "Calculus",
    "section": "",
    "text": "For indeterminate forms \\(\\frac{0}{0}\\) or \\(\\frac{\\infty}{\\infty}\\):\n\\[\n\\lim_{x \\to c} \\frac{f(x)}{g(x)} = \\lim_{x \\to c} \\frac{f'(x)}{g'(x)}\n\\]\nML relevance: Analyzing limiting behavior of loss functions"
  },
  {
    "objectID": "calculus.html#common-activation-function-derivatives",
    "href": "calculus.html#common-activation-function-derivatives",
    "title": "Calculus",
    "section": "",
    "text": "Sigmoid: \\(\\sigma(x) = \\frac{1}{1 + e^{-x}}\\)\n\\[\n\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))\n\\]\nTanh: \\(\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\\)\n\\[\n\\tanh'(x) = 1 - \\tanh^2(x)\n\\]\nReLU: \\(\\text{ReLU}(x) = \\max(0, x)\\)\n\\[\n\\text{ReLU}'(x) = \\begin{cases} 1 & \\text{if } x &gt; 0 \\\\ 0 & \\text{if } x &lt; 0 \\\\ \\text{undefined} & \\text{if } x = 0 \\end{cases}\n\\]\nSoftmax: For vector \\(\\mathbf{z}\\), \\(\\text{softmax}(\\mathbf{z})_i = \\frac{e^{z_i}}{\\sum_j e^{z_j}}\\)\n\\[\n\\frac{\\partial \\text{softmax}(\\mathbf{z})_i}{\\partial z_j} = \\text{softmax}(\\mathbf{z})_i (\\delta_{ij} - \\text{softmax}(\\mathbf{z})_j)\n\\]\nwhere \\(\\delta_{ij}\\) is the Kronecker delta"
  },
  {
    "objectID": "calculus.html#gradient-descent",
    "href": "calculus.html#gradient-descent",
    "title": "Calculus",
    "section": "2.1 Gradient Descent",
    "text": "2.1 Gradient Descent\nUpdate rule:\n\\[\n\\mathbf{x}_{t+1} = \\mathbf{x}_t - \\alpha \\nabla f(\\mathbf{x}_t)\n\\]\nwhere \\(\\alpha\\) is the learning rate\nVariants: - Stochastic Gradient Descent (SGD): Use gradient from single sample - Mini-batch GD: Use gradient from small batch - Momentum: \\(\\mathbf{v}_{t+1} = \\beta \\mathbf{v}_t + \\nabla f(\\mathbf{x}_t)\\)"
  },
  {
    "objectID": "calculus.html#backpropagation",
    "href": "calculus.html#backpropagation",
    "title": "Calculus",
    "section": "2.2 Backpropagation",
    "text": "2.2 Backpropagation\nChain rule application:\nFor loss \\(L\\) and layer outputs \\(\\mathbf{z}^{(l)}\\):\n\\[\n\\frac{\\partial L}{\\partial \\mathbf{w}^{(l)}} = \\frac{\\partial L}{\\partial \\mathbf{z}^{(l)}} \\cdot \\frac{\\partial \\mathbf{z}^{(l)}}{\\partial \\mathbf{w}^{(l)}}\n\\]\nRecursive gradient flow:\n\\[\n\\frac{\\partial L}{\\partial \\mathbf{z}^{(l)}} = \\frac{\\partial L}{\\partial \\mathbf{z}^{(l+1)}} \\cdot \\frac{\\partial \\mathbf{z}^{(l+1)}}{\\partial \\mathbf{z}^{(l)}}\n\\]"
  },
  {
    "objectID": "calculus.html#loss-functions-and-their-derivatives",
    "href": "calculus.html#loss-functions-and-their-derivatives",
    "title": "Calculus",
    "section": "2.3 Loss Functions and Their Derivatives",
    "text": "2.3 Loss Functions and Their Derivatives\nMean Squared Error (MSE):\n\\[\nL = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n\\]\n\\[\n\\frac{\\partial L}{\\partial \\hat{y}_i} = -\\frac{2}{n}(y_i - \\hat{y}_i)\n\\]\nCross-Entropy Loss (binary):\n\\[\nL = -\\frac{1}{n}\\sum_{i=1}^n [y_i \\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i)]\n\\]\n\\[\n\\frac{\\partial L}{\\partial \\hat{y}_i} = -\\frac{1}{n}\\left[\\frac{y_i}{\\hat{y}_i} - \\frac{1-y_i}{1-\\hat{y}_i}\\right]\n\\]\nCategorical Cross-Entropy (with softmax):\n\\[\nL = -\\sum_{i=1}^n y_i \\log(\\hat{y}_i)\n\\]\nCombined softmax + cross-entropy derivative simplifies to:\n\\[\n\\frac{\\partial L}{\\partial z_i} = \\hat{y}_i - y_i\n\\]"
  },
  {
    "objectID": "calculus.html#regularization-terms",
    "href": "calculus.html#regularization-terms",
    "title": "Calculus",
    "section": "2.4 Regularization Terms",
    "text": "2.4 Regularization Terms\nL2 Regularization (Ridge):\n\\[\nR(\\mathbf{w}) = \\frac{\\lambda}{2}\\|\\mathbf{w}\\|_2^2 = \\frac{\\lambda}{2}\\sum_i w_i^2\n\\]\n\\[\n\\frac{\\partial R}{\\partial w_i} = \\lambda w_i\n\\]\nL1 Regularization (Lasso):\n\\[\nR(\\mathbf{w}) = \\lambda\\|\\mathbf{w}\\|_1 = \\lambda\\sum_i |w_i|\n\\]\n\\[\n\\frac{\\partial R}{\\partial w_i} = \\lambda \\cdot \\text{sign}(w_i)\n\\]"
  },
  {
    "objectID": "calculus.html#quick-reference-table",
    "href": "calculus.html#quick-reference-table",
    "title": "Calculus",
    "section": "2.5 Quick Reference Table",
    "text": "2.5 Quick Reference Table\n\n\n\n\n\n\n\n\nConcept\nFormula\nML Application\n\n\n\n\nGradient\n\\(\\nabla f\\)\nGradient descent direction\n\n\nJacobian\n\\(J_{ij} = \\frac{\\partial f_i}{\\partial x_j}\\)\nBackpropagation\n\n\nHessian\n\\(H_{ij} = \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}\\)\nSecond-order optimization\n\n\nChain Rule\n\\(\\frac{df}{dx} = \\frac{df}{dg} \\cdot \\frac{dg}{dx}\\)\nBackpropagation\n\n\nTaylor Expansion\n\\(f(x+\\Delta x) \\approx f(x) + f'(x)\\Delta x\\)\nLocal approximation\n\n\nConvexity\n\\(f''(x) \\geq 0\\)\nOptimization guarantees\n\n\nLagrange Multipliers\n\\(\\nabla f = \\lambda \\nabla g\\)\nConstrained optimization\n\n\nSigmoid Derivative\n\\(\\sigma'(x) = \\sigma(x)(1-\\sigma(x))\\)\nNeural network activations"
  },
  {
    "objectID": "linear-algebra.html",
    "href": "linear-algebra.html",
    "title": "Linear Algebra",
    "section": "",
    "text": "This section covers essential linear concepts that serve as the mathematical foundation for machine learning and deep learning.\n\n\n\nVector: A 1D array of numbers, often used to represent features.\nMatrix: A 2D array of numbers, common for representing datasets and weights.\n\nExample:\n\\[\n\\mathbf{x} = \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix}, \\quad\n\\mathbf{W} = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\n\\]\n\n\n\nUsed in the forward pass of neural networks:\n\\[\n\\mathbf{y} = \\mathbf{W}\\mathbf{x}\n\\]\n\n\n\nMeasures projection or similarity:\n\\[\n\\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i} a_i b_i\n\\]\nGeometric interpretation:\n\\[\n\\mathbf{a} \\cdot \\mathbf{b} = \\|\\mathbf{a}\\| \\|\\mathbf{b}\\| \\cos(\\theta)\n\\]\n\nUsed in cosine similarity for comparing vectors\n\n\n\n\n\nL2 norm (Euclidean length): \\[\n\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_i x_i^2}\n\\]\nL1 norm (Manhattan distance): \\[\n\\|\\mathbf{x}\\|_1 = \\sum_i |x_i|\n\\]\nFrobenius norm (for matrices): \\[\n\\|\\mathbf{A}\\|_F = \\sqrt{\\sum_{i,j} A_{ij}^2}\n\\]\n\nUsed to compute distances and regularization penalties.\n\n\n\nThe determinant of a square matrix \\(\\mathbf{A}\\) is a scalar value:\n\\[\n\\det(\\mathbf{A})\n\\]\n\n\\(\\det(\\mathbf{A}) = 0\\) means matrix is singular (non-invertible)\n\\(|\\det(\\mathbf{A})|\\) represents volume scaling factor\nUsed in linear transformations and invertibility checks\n\n\n\n\nThe trace of a square matrix is the sum of diagonal elements:\n\\[\n\\text{Tr}(\\mathbf{A}) = \\sum_i A_{ii}\n\\]\nProperties:\n\n\\(\\text{Tr}(\\mathbf{A} + \\mathbf{B}) = \\text{Tr}(\\mathbf{A}) + \\text{Tr}(\\mathbf{B})\\)\n\\(\\text{Tr}(\\mathbf{AB}) = \\text{Tr}(\\mathbf{BA})\\) (cyclic property)\n\\(\\text{Tr}(\\mathbf{A}) = \\sum_i \\lambda_i\\) (sum of eigenvalues)\n\n\n\n\nUsed in PCA (dimensionality reduction):\n\\[\n\\mathbf{A}\\mathbf{v} = \\lambda\\mathbf{v}\n\\]\nWhere:\n\n\\(\\mathbf{v}\\) is an eigenvector\n\\(\\lambda\\) is the corresponding eigenvalue\nCharacteristic equation: \\(\\det(\\mathbf{A} - \\lambda\\mathbf{I}) = 0\\)\n\nProperties:\n\n\\(\\text{Tr}(\\mathbf{A}) = \\sum_i \\lambda_i\\) (sum of eigenvalues)\n\\(\\det(\\mathbf{A}) = \\prod_i \\lambda_i\\) (product of eigenvalues)\n\n\n\n\nUsed in matrix factorization and recommender systems:\n\\[\n\\mathbf{A} = \\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^T\n\\]\nWhere:\n\n\\(\\mathbf{U}\\): left singular vectors (m × m orthogonal)\n\\(\\mathbf{\\Sigma}\\): diagonal matrix of singular values (m × n)\n\\(\\mathbf{V}\\): right singular vectors (n × n orthogonal)\n\nKey properties:\n\nAlways exists (unlike eigendecomposition)\nSingular values \\(\\sigma_i \\geq 0\\), ordered by magnitude\nLow-rank approximation: keep top \\(k\\) singular values\n\nApplications:\n\nDimensionality reduction (truncated SVD)\nImage compression\nCollaborative filtering\nPseudo-inverse calculation\n\n\n\n\n\nRank: The dimension of the column space (or row space):\n\\[\n\\text{rank}(\\mathbf{A}) = \\text{number of linearly independent columns}\n\\]\nProperties:\n\n\\(\\text{rank}(\\mathbf{A}) \\leq \\min(m, n)\\) for \\(m \\times n\\) matrix\nFull rank: \\(\\text{rank}(\\mathbf{A}) = \\min(m, n)\\)\nRank-deficient: \\(\\text{rank}(\\mathbf{A}) &lt; \\min(m, n)\\)\n\nInvertibility:\n\nSquare matrix \\(\\mathbf{A}\\) is invertible \\(\\Leftrightarrow\\) \\(\\text{rank}(\\mathbf{A}) = n\\)\n\\(\\mathbf{A}^{-1}\\) exists \\(\\Leftrightarrow\\) \\(\\det(\\mathbf{A}) \\neq 0\\)\nFor invertible \\(\\mathbf{A}\\): \\(\\mathbf{AA}^{-1} = \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}\\)\n\nML relevance: Feature redundancy, linear dependence in datasets\n\n\n\nOrthogonal vectors: \\(\\mathbf{x} \\perp \\mathbf{y}\\) if \\(\\mathbf{x}^T\\mathbf{y} = 0\\)\nOrthogonal matrix \\(\\mathbf{Q}\\):\n\n\\(\\mathbf{Q}^T\\mathbf{Q} = \\mathbf{QQ}^T = \\mathbf{I}\\)\nColumns are orthonormal\nPreserves lengths: \\(\\|\\mathbf{Qx}\\| = \\|\\mathbf{x}\\|\\)\n\nProjection matrix \\(\\mathbf{P}\\):\nProjection of \\(\\mathbf{b}\\) onto column space of \\(\\mathbf{A}\\):\n\\[\n\\mathbf{P} = \\mathbf{A}(\\mathbf{A}^T\\mathbf{A})^{-1}\\mathbf{A}^T\n\\]\nProperties:\n\n\\(\\mathbf{P}^2 = \\mathbf{P}\\) (idempotent)\n\\(\\mathbf{P}^T = \\mathbf{P}\\) (symmetric)\n\nQR Decomposition:\n\\[\n\\mathbf{A} = \\mathbf{QR}\n\\]\nWhere \\(\\mathbf{Q}\\) is orthogonal, \\(\\mathbf{R}\\) is upper triangular\nML relevance:\n\nLinear regression (least squares)\nGram-Schmidt orthogonalization\nSolving linear systems\n\n\n\n\nA symmetric matrix \\(\\mathbf{A}\\) is positive definite if:\n\\[\n\\mathbf{x}^T\\mathbf{A}\\mathbf{x} &gt; 0 \\text{ for all } \\mathbf{x} \\neq \\mathbf{0}\n\\]\nEquivalent conditions:\n\nAll eigenvalues \\(\\lambda_i &gt; 0\\)\nAll leading principal minors \\(&gt; 0\\)\n\\(\\mathbf{A} = \\mathbf{B}^T\\mathbf{B}\\) for some invertible \\(\\mathbf{B}\\)\n\nPositive semi-definite: \\(\\mathbf{x}^T\\mathbf{A}\\mathbf{x} \\geq 0\\) (allows zero eigenvalues)\nML relevance:\n\nCovariance matrices (always positive semi-definite)\nHessian matrices in optimization\nConvex optimization (positive definite Hessian → strict convexity)\nConvergence guarantees\n\n\n\n\nGradient (scalar function w.r.t vector):\n\\[\n\\nabla_\\mathbf{x} f(\\mathbf{x}) = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_n} \\end{bmatrix}\n\\]\nJacobian (vector function w.r.t vector):\n\\[\n\\mathbf{J} = \\begin{bmatrix} \\frac{\\partial f_i}{\\partial x_j} \\end{bmatrix} \\text{ (m × n matrix)}\n\\]\nCommon derivatives:\n\n\\(\\nabla_\\mathbf{x}(\\mathbf{a}^T\\mathbf{x}) = \\mathbf{a}\\)\n\\(\\nabla_\\mathbf{x}(\\mathbf{x}^T\\mathbf{A}\\mathbf{x}) = (\\mathbf{A} + \\mathbf{A}^T)\\mathbf{x}\\)\nFor symmetric \\(\\mathbf{A}\\): \\(\\nabla_\\mathbf{x}(\\mathbf{x}^T\\mathbf{A}\\mathbf{x}) = 2\\mathbf{A}\\mathbf{x}\\)\n\\(\\nabla_\\mathbf{x}(\\|\\mathbf{x}\\|^2) = 2\\mathbf{x}\\)\n\nHessian (second derivatives):\n\\[\n\\mathbf{H} = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_i \\partial x_j} \\end{bmatrix}\n\\]\nML relevance:\n\nBackpropagation\nGradient descent optimization\nNeural network training\n\n\n\n\nFor symmetric matrix \\(\\mathbf{A}\\):\n\\[\n\\mathbf{A} = \\mathbf{Q}\\mathbf{\\Lambda}\\mathbf{Q}^T\n\\]\nWhere:\n\n\\(\\mathbf{Q}\\): orthogonal matrix of eigenvectors\n\\(\\mathbf{\\Lambda}\\): diagonal matrix of eigenvalues\n\\(\\mathbf{Q}^T = \\mathbf{Q}^{-1}\\)\n\nProperties:\n\nAll eigenvalues are real\nEigenvectors are orthogonal\nCan always be diagonalized\n\nML relevance:\n\nPrincipal Component Analysis (PCA)\nCovariance matrix diagonalization\nQuadratic forms"
  },
  {
    "objectID": "linear-algebra.html#vectors-and-matrices",
    "href": "linear-algebra.html#vectors-and-matrices",
    "title": "Linear Algebra",
    "section": "",
    "text": "Vector: A 1D array of numbers, often used to represent features.\nMatrix: A 2D array of numbers, common for representing datasets and weights.\n\nExample:\n\\[\n\\mathbf{x} = \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix}, \\quad\n\\mathbf{W} = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "linear-algebra.html#matrix-multiplication",
    "href": "linear-algebra.html#matrix-multiplication",
    "title": "Linear Algebra",
    "section": "",
    "text": "Used in the forward pass of neural networks:\n\\[\n\\mathbf{y} = \\mathbf{W}\\mathbf{x}\n\\]"
  },
  {
    "objectID": "linear-algebra.html#dot-product",
    "href": "linear-algebra.html#dot-product",
    "title": "Linear Algebra",
    "section": "",
    "text": "Measures projection or similarity:\n\\[\n\\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i} a_i b_i\n\\]\nGeometric interpretation:\n\\[\n\\mathbf{a} \\cdot \\mathbf{b} = \\|\\mathbf{a}\\| \\|\\mathbf{b}\\| \\cos(\\theta)\n\\]\n\nUsed in cosine similarity for comparing vectors"
  },
  {
    "objectID": "linear-algebra.html#norms-and-distances",
    "href": "linear-algebra.html#norms-and-distances",
    "title": "Linear Algebra",
    "section": "",
    "text": "L2 norm (Euclidean length): \\[\n\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_i x_i^2}\n\\]\nL1 norm (Manhattan distance): \\[\n\\|\\mathbf{x}\\|_1 = \\sum_i |x_i|\n\\]\nFrobenius norm (for matrices): \\[\n\\|\\mathbf{A}\\|_F = \\sqrt{\\sum_{i,j} A_{ij}^2}\n\\]\n\nUsed to compute distances and regularization penalties."
  },
  {
    "objectID": "linear-algebra.html#determinant",
    "href": "linear-algebra.html#determinant",
    "title": "Linear Algebra",
    "section": "",
    "text": "The determinant of a square matrix \\(\\mathbf{A}\\) is a scalar value:\n\\[\n\\det(\\mathbf{A})\n\\]\n\n\\(\\det(\\mathbf{A}) = 0\\) means matrix is singular (non-invertible)\n\\(|\\det(\\mathbf{A})|\\) represents volume scaling factor\nUsed in linear transformations and invertibility checks"
  },
  {
    "objectID": "linear-algebra.html#trace",
    "href": "linear-algebra.html#trace",
    "title": "Linear Algebra",
    "section": "",
    "text": "The trace of a square matrix is the sum of diagonal elements:\n\\[\n\\text{Tr}(\\mathbf{A}) = \\sum_i A_{ii}\n\\]\nProperties:\n\n\\(\\text{Tr}(\\mathbf{A} + \\mathbf{B}) = \\text{Tr}(\\mathbf{A}) + \\text{Tr}(\\mathbf{B})\\)\n\\(\\text{Tr}(\\mathbf{AB}) = \\text{Tr}(\\mathbf{BA})\\) (cyclic property)\n\\(\\text{Tr}(\\mathbf{A}) = \\sum_i \\lambda_i\\) (sum of eigenvalues)"
  },
  {
    "objectID": "linear-algebra.html#eigenvalues-and-eigenvectors",
    "href": "linear-algebra.html#eigenvalues-and-eigenvectors",
    "title": "Linear Algebra",
    "section": "",
    "text": "Used in PCA (dimensionality reduction):\n\\[\n\\mathbf{A}\\mathbf{v} = \\lambda\\mathbf{v}\n\\]\nWhere:\n\n\\(\\mathbf{v}\\) is an eigenvector\n\\(\\lambda\\) is the corresponding eigenvalue\nCharacteristic equation: \\(\\det(\\mathbf{A} - \\lambda\\mathbf{I}) = 0\\)\n\nProperties:\n\n\\(\\text{Tr}(\\mathbf{A}) = \\sum_i \\lambda_i\\) (sum of eigenvalues)\n\\(\\det(\\mathbf{A}) = \\prod_i \\lambda_i\\) (product of eigenvalues)"
  },
  {
    "objectID": "linear-algebra.html#singular-value-decomposition-svd",
    "href": "linear-algebra.html#singular-value-decomposition-svd",
    "title": "Linear Algebra",
    "section": "",
    "text": "Used in matrix factorization and recommender systems:\n\\[\n\\mathbf{A} = \\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^T\n\\]\nWhere:\n\n\\(\\mathbf{U}\\): left singular vectors (m × m orthogonal)\n\\(\\mathbf{\\Sigma}\\): diagonal matrix of singular values (m × n)\n\\(\\mathbf{V}\\): right singular vectors (n × n orthogonal)\n\nKey properties:\n\nAlways exists (unlike eigendecomposition)\nSingular values \\(\\sigma_i \\geq 0\\), ordered by magnitude\nLow-rank approximation: keep top \\(k\\) singular values\n\nApplications:\n\nDimensionality reduction (truncated SVD)\nImage compression\nCollaborative filtering\nPseudo-inverse calculation"
  },
  {
    "objectID": "linear-algebra.html#matrix-rank-and-invertibility",
    "href": "linear-algebra.html#matrix-rank-and-invertibility",
    "title": "Linear Algebra",
    "section": "",
    "text": "Rank: The dimension of the column space (or row space):\n\\[\n\\text{rank}(\\mathbf{A}) = \\text{number of linearly independent columns}\n\\]\nProperties:\n\n\\(\\text{rank}(\\mathbf{A}) \\leq \\min(m, n)\\) for \\(m \\times n\\) matrix\nFull rank: \\(\\text{rank}(\\mathbf{A}) = \\min(m, n)\\)\nRank-deficient: \\(\\text{rank}(\\mathbf{A}) &lt; \\min(m, n)\\)\n\nInvertibility:\n\nSquare matrix \\(\\mathbf{A}\\) is invertible \\(\\Leftrightarrow\\) \\(\\text{rank}(\\mathbf{A}) = n\\)\n\\(\\mathbf{A}^{-1}\\) exists \\(\\Leftrightarrow\\) \\(\\det(\\mathbf{A}) \\neq 0\\)\nFor invertible \\(\\mathbf{A}\\): \\(\\mathbf{AA}^{-1} = \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}\\)\n\nML relevance: Feature redundancy, linear dependence in datasets"
  },
  {
    "objectID": "linear-algebra.html#orthogonality-and-projections",
    "href": "linear-algebra.html#orthogonality-and-projections",
    "title": "Linear Algebra",
    "section": "",
    "text": "Orthogonal vectors: \\(\\mathbf{x} \\perp \\mathbf{y}\\) if \\(\\mathbf{x}^T\\mathbf{y} = 0\\)\nOrthogonal matrix \\(\\mathbf{Q}\\):\n\n\\(\\mathbf{Q}^T\\mathbf{Q} = \\mathbf{QQ}^T = \\mathbf{I}\\)\nColumns are orthonormal\nPreserves lengths: \\(\\|\\mathbf{Qx}\\| = \\|\\mathbf{x}\\|\\)\n\nProjection matrix \\(\\mathbf{P}\\):\nProjection of \\(\\mathbf{b}\\) onto column space of \\(\\mathbf{A}\\):\n\\[\n\\mathbf{P} = \\mathbf{A}(\\mathbf{A}^T\\mathbf{A})^{-1}\\mathbf{A}^T\n\\]\nProperties:\n\n\\(\\mathbf{P}^2 = \\mathbf{P}\\) (idempotent)\n\\(\\mathbf{P}^T = \\mathbf{P}\\) (symmetric)\n\nQR Decomposition:\n\\[\n\\mathbf{A} = \\mathbf{QR}\n\\]\nWhere \\(\\mathbf{Q}\\) is orthogonal, \\(\\mathbf{R}\\) is upper triangular\nML relevance:\n\nLinear regression (least squares)\nGram-Schmidt orthogonalization\nSolving linear systems"
  },
  {
    "objectID": "linear-algebra.html#positive-definite-matrices",
    "href": "linear-algebra.html#positive-definite-matrices",
    "title": "Linear Algebra",
    "section": "",
    "text": "A symmetric matrix \\(\\mathbf{A}\\) is positive definite if:\n\\[\n\\mathbf{x}^T\\mathbf{A}\\mathbf{x} &gt; 0 \\text{ for all } \\mathbf{x} \\neq \\mathbf{0}\n\\]\nEquivalent conditions:\n\nAll eigenvalues \\(\\lambda_i &gt; 0\\)\nAll leading principal minors \\(&gt; 0\\)\n\\(\\mathbf{A} = \\mathbf{B}^T\\mathbf{B}\\) for some invertible \\(\\mathbf{B}\\)\n\nPositive semi-definite: \\(\\mathbf{x}^T\\mathbf{A}\\mathbf{x} \\geq 0\\) (allows zero eigenvalues)\nML relevance:\n\nCovariance matrices (always positive semi-definite)\nHessian matrices in optimization\nConvex optimization (positive definite Hessian → strict convexity)\nConvergence guarantees"
  },
  {
    "objectID": "linear-algebra.html#matrix-calculus-basics",
    "href": "linear-algebra.html#matrix-calculus-basics",
    "title": "Linear Algebra",
    "section": "",
    "text": "Gradient (scalar function w.r.t vector):\n\\[\n\\nabla_\\mathbf{x} f(\\mathbf{x}) = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_n} \\end{bmatrix}\n\\]\nJacobian (vector function w.r.t vector):\n\\[\n\\mathbf{J} = \\begin{bmatrix} \\frac{\\partial f_i}{\\partial x_j} \\end{bmatrix} \\text{ (m × n matrix)}\n\\]\nCommon derivatives:\n\n\\(\\nabla_\\mathbf{x}(\\mathbf{a}^T\\mathbf{x}) = \\mathbf{a}\\)\n\\(\\nabla_\\mathbf{x}(\\mathbf{x}^T\\mathbf{A}\\mathbf{x}) = (\\mathbf{A} + \\mathbf{A}^T)\\mathbf{x}\\)\nFor symmetric \\(\\mathbf{A}\\): \\(\\nabla_\\mathbf{x}(\\mathbf{x}^T\\mathbf{A}\\mathbf{x}) = 2\\mathbf{A}\\mathbf{x}\\)\n\\(\\nabla_\\mathbf{x}(\\|\\mathbf{x}\\|^2) = 2\\mathbf{x}\\)\n\nHessian (second derivatives):\n\\[\n\\mathbf{H} = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_i \\partial x_j} \\end{bmatrix}\n\\]\nML relevance:\n\nBackpropagation\nGradient descent optimization\nNeural network training"
  },
  {
    "objectID": "linear-algebra.html#spectral-theorem",
    "href": "linear-algebra.html#spectral-theorem",
    "title": "Linear Algebra",
    "section": "",
    "text": "For symmetric matrix \\(\\mathbf{A}\\):\n\\[\n\\mathbf{A} = \\mathbf{Q}\\mathbf{\\Lambda}\\mathbf{Q}^T\n\\]\nWhere:\n\n\\(\\mathbf{Q}\\): orthogonal matrix of eigenvectors\n\\(\\mathbf{\\Lambda}\\): diagonal matrix of eigenvalues\n\\(\\mathbf{Q}^T = \\mathbf{Q}^{-1}\\)\n\nProperties:\n\nAll eigenvalues are real\nEigenvectors are orthogonal\nCan always be diagonalized\n\nML relevance:\n\nPrincipal Component Analysis (PCA)\nCovariance matrix diagonalization\nQuadratic forms"
  }
]